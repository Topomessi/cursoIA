{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorando métodos y métricas de distancia para la agrupación de series de tiempo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastdtw'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4f044e51ce5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfastdtw\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfastdtw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastdtw'"
     ]
    }
   ],
   "source": [
    "# import of libraries we will use\n",
    "%matplotlib inline\n",
    "import random\n",
    "from math import sqrt, log, floor\n",
    "from fastdtw import fastdtw\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import euclidean\n",
    "import scipy.cluster.hierarchy\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sns.set(style='white')\n",
    "# \"fix\" the randomness for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conjunto de datos se compone de alrededor de 200 series temporales multidimensionales. Cada serie contiene medidas tomadas por 23 sensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('../data/train.txt', header=None, delim_whitespace=True)\n",
    "# data normalization\n",
    "for column in range(2, 26):\n",
    "    data[column] = (data[column] - data[column].mean()) / data[column].std()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "La duración media de la serie ronda los 210 pasos de tiempo. La longitud mínima es de 128 pasos de tiempo y el máximo es de 357 pasos de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = data[data[0]==43]\n",
    "for i in range(2, 26):\n",
    "    (serie[i]+5*i).plot(x=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que muchos sensores miden cosas similares y hay mucho ruido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Al principio, intentamos agrupar series de tiempo completas. Con el fin de evaluar los resultados de los diferentes algoritmos de agrupamiento, generamos un conjunto de entrenamiento con etiquetas de verdad del terreno cortando una cola de longitud aleatoria de cada serie temporal. La etiqueta de una serie es entonces la longitud de la cola cortada. Probamos los algoritmos de agrupamiento de K-means y HAC con distancia euclidiana y DTW. La evaluación que utiliza información mutua normalizada y un índice de rand ajustado ha mostrado un rendimiento deficiente. Atribuimos estos resultados a un ruido significativo en los datos y al hecho de que las series tienen longitudes muy diferentes. Esto ha llevado a nuestro segundo enfoque, agrupar solo fragmentos de series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupando pedazos de series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea es cortar series en trozos separados de igual longitud y asignarles etiquetas en función de la distancia desde el final de la serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data by each timeserie\n",
    "grouped = data.groupby(0)\n",
    "series = [group.drop(labels=[0,1], axis=1) for name, group in grouped]\n",
    "# number of clusters\n",
    "k = 6\n",
    "# length of chunks\n",
    "chunk_length = 10\n",
    "# go through groups and compute chunks and labels\n",
    "train_set = []\n",
    "true_labels = []\n",
    "for serie in series:\n",
    "    reversed_serie = serie.iloc[::-1]\n",
    "    for i in range(len(serie) // chunk_length):        \n",
    "        chunk = reversed_serie[i*10:(i+1)*10]\n",
    "        true_label = i\n",
    "        train_set.append(chunk)\n",
    "        true_labels.append(true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Algunos algoritmos son lentos, por lo que se podria muestrear solo un subconjunto de fragmentos para ejecutar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "n = 100\n",
    "indices = random.sample(list(range(len(train_set))), k=n) \n",
    "sample = [train_set[i] for i in indices]\n",
    "sample_labels = [true_labels[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculando distancias entre trozos, se pueden evaluar métricas de distancia, distancia Euclidiana, RMS, DTW . Como los trozos son de la misma longitud, RMS es solo una versión escalada de la distancia euclédica que da como resultado grupos muy similares o idénticos. Los cálculos de la distancia Euclidiana y RMS son bastante rápidos, mientras que DTW se ejecuta un poco más lento y LCSS fue el más lento. Tomaría más de una hora calcular las distancias de cada par de trozos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute eucledean distance\n",
    "euclidean_distances = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        euclidean_distances[i,j] = euclidean(sample[i].values.flatten(),\n",
    "                                             sample[j].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distances using RMSE\n",
    "rmse_distances = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        rmse_distances[i,j] = sqrt(mean_squared_error(sample[i].values.flatten(),\n",
    "                                                      sample[j].values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.heatmap(rmse_distances, xticklabels=False, yticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distances using DTW\n",
    "def dtw(x, y):\n",
    "    dist, _ = fastdtw(x, y, dist=euclidean)\n",
    "    return dist\n",
    "\n",
    "dtw_distances = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        dtw_distances[i,j] = dtw(sample[i], sample[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada una de estas métricas, se prueban lo algoritmos de agrupación, K-medias y agrupación jerárquica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "y_pred_km_euc = KMeans(n_clusters=k).fit_predict(euclidean_distances)\n",
    "y_pred_km_dtw = KMeans(n_clusters=k).fit_predict(dtw_distances)\n",
    "y_pred_km_rmse = KMeans(n_clusters=k).fit_predict(rmse_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hac_euc = AgglomerativeClustering(n_clusters=k)\\\n",
    "                 .fit_predict(euclidean_distances)\n",
    "y_pred_hac_dtw = AgglomerativeClustering(n_clusters=k).fit_predict(dtw_distances)\n",
    "y_pred_hac_rmse = AgglomerativeClustering(n_clusters=k).fit_predict(rmse_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True label normalisation\n",
    "log_base = max(true_labels)**(1 / k) + 0.05\n",
    "normalised_true_labels = [floor(log(l+1, log_base)) for l in sample_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Normalized Mutual Information')\n",
    "print('K-means + Eucledean: ', normalized_mutual_info_score(normalised_true_labels,\n",
    "                                                            y_pred_km_euc))\n",
    "print('K-means + RMS:       ', normalized_mutual_info_score(normalised_true_labels,\n",
    "                                                            y_pred_km_rmse))\n",
    "print('K-means + DTW:       ', normalized_mutual_info_score(normalised_true_labels,\n",
    "                                                            y_pred_km_dtw))\n",
    "print('HAC + Eucledean:     ', normalized_mutual_info_score(normalised_true_labels,\n",
    "                                                            y_pred_hac_euc))\n",
    "print('HAC + RMS:           ', normalized_mutual_info_score(normalised_true_labels,\n",
    "                                                            y_pred_hac_rmse))\n",
    "print('HAC + DTW:           ', normalized_mutual_info_score(normalised_true_labels,\n",
    "                                                            y_pred_hac_dtw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adjusted Rand Index')\n",
    "print('K-means + Euclidean:  ', adjusted_rand_score(normalised_true_labels,\n",
    "                                                    y_pred_km_euc))\n",
    "print('K-means + RMS:       ', adjusted_rand_score(normalised_true_labels,\n",
    "                                                   y_pred_km_rmse))\n",
    "print('K-means + DTW:       ', adjusted_rand_score(normalised_true_labels,\n",
    "                                                   y_pred_km_dtw))\n",
    "print('HAC + Euclidean:     ', adjusted_rand_score(normalised_true_labels,\n",
    "                                                   y_pred_hac_euc))\n",
    "print('HAC + RMS:           ', adjusted_rand_score(normalised_true_labels,\n",
    "                                                   y_pred_hac_rmse))\n",
    "print('HAC + DTW:           ', adjusted_rand_score(normalised_true_labels,\n",
    "                                                   y_pred_hac_dtw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias \n",
    "https://github.com/effa/time-series-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
