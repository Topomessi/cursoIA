{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdNN6e45I4Un"
   },
   "source": [
    " Hugging Face Tokenizers\n",
    "\n",
    "Tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation. Consider how the program might break up the following sentences into words.\n",
    "\n",
    "* This is a test.\n",
    "* Ok, but what about this?\n",
    "* Is U.S.A. the same as USA.?\n",
    "* What is the best data-set to use?\n",
    "* I think I will do this-no wait; I will do that.\n",
    "\n",
    "The hugging face includes tokenizers that can break these sentences into words and subwords. Because English, and some other languages, are made up of common word parts, we tokenize subwords. For example, a gerund word, such as \"sleeping,\" will be tokenized into \"sleep\" and \"##ing\".\n",
    "\n",
    "We begin by installing Hugging Face if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wa1ncodn8y0r"
   },
   "source": [
    "First, we create a Hugging Face tokenizer. There are several different tokenizers available from the Hugging Face hub. For this example, we will make use of the following tokenizer:\n",
    "\n",
    "* distilbert-base-uncased\n",
    "\n",
    "This tokenizer is based on BERT and assumes case-insensitive English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kSGtW0E7xcK9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fcd21ba30d4237ade251b2d478e1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UserFiles\\anaconda\\envs\\ia\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yoda\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5ae2c4e5a44338945cda10c6d8ea43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180a87ab1cac4ce0991b151019844f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57bc4504b4b4b21ae2a9ee2e604ee65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YofjyJw59U2x"
   },
   "source": [
    "We can now tokenize a sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVCmyao2zLQ3",
    "outputId": "6b33031a-8667-4431-aae2-26595195d69d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 3733, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer('Tokenizing text is easy.')\n",
    "print(encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QgmlpyezhMy"
   },
   "source": [
    "The result of this tokenization contains two elements:\n",
    "* input_ids - The individual subword indexes, each index uniquely identifies a subword.\n",
    "* attention_mask - Which values in *input_ids* are meaningful and not  padding.\n",
    "This sentence had no padding, so all elements have an attention mask of \"1\". Later, we will request the output to be of a fixed length, introducing padding, which always has an attention mask of \"0\". Though each tokenizer can be implemented differently, the attention mask of a tokenizer is generally either \"0\" or \"1\". \n",
    "\n",
    "Due to subwords and special tokens, the number of tokens may not match the number of words in the source string. We can see the meanings of the individual tokens by converting these IDs back to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ww3XPc-i2Y6c",
    "outputId": "6179b416-61cd-4142-fa15-ae2296e9eab7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'token', '##izing', 'text', 'is', 'easy', '.', '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoded.input_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KM7VRJECoGU"
   },
   "source": [
    "As you can see, there are two special tokens placed at the beginning and end of each sequence. We will soon see how we can include or exclude these special tokens. These special tokens can vary per tokenizer; however, [CLS] begins a sequence for this tokenizer, and [SEP] ends a sequence. You will also see that the gerund \"tokening\" is broken into \"token\" and \"*ing\".\n",
    "\n",
    "For this tokenizer, the special tokens occur between 100 and 103. Most Hugging Face tokenizers use this approximate range for special tokens. The value zero (0) typically represents padding. We can display all special tokens with this command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtQiOmSl2rXt",
    "outputId": "6f5f2aef-d718-4675-d0a5-9c6c9d960c87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0, 100, 101, 102, 103])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nQ-r6bz3ESN"
   },
   "source": [
    "This tokenizer supports these common tokens:\n",
    "\n",
    "* \\[CLS\\] - Sequence beginning.\n",
    "* \\[SEP\\] - Sequence end.\n",
    "* \\[PAD\\] - Padding.\n",
    "* \\[UNK\\] - Unknown token.\n",
    "* \\[MASK\\] - Mask out tokens for a neural network to predict. Not used in this book, see [MLM paper](https://arxiv.org/abs/2109.01819). \n",
    "\n",
    "It is also possible to tokenize lists of sequences. We can pad and truncate sequences to achieve a standard length by tokenizing many sequences at once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TI4RZXhc4v9k",
    "outputId": "2ae7f2f9-3d8a-421e-c9b9-4dd933817cd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Input IDs**\n",
      "[101, 2023, 3185, 2001, 2307, 999, 102, 0, 0, 0, 0]\n",
      "[101, 1045, 6283, 2023, 2693, 1010, 5949, 1997, 2051, 999, 102]\n",
      "[101, 8680, 1029, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "**Attention Mask**\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "text = [\n",
    "    \"This movie was great!\",\n",
    "    \"I hated this move, waste of time!\",\n",
    "    \"Epic?\"\n",
    "]\n",
    "\n",
    "encoded = tokenizer(text, padding=True, add_special_tokens=True)\n",
    "\n",
    "print(\"**Input IDs**\")\n",
    "for a in encoded.input_ids:\n",
    "    print(a)\n",
    "\n",
    "print(\"**Attention Mask**\")\n",
    "for a in encoded.attention_mask:\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rIF8TEEF9C-"
   },
   "source": [
    "Notice the **input_id**'s for the three movie review text sequences. Each of these sequences begins with 101 and we pad with zeros. Just before the padding, each group of IDs ends with 102. The attention masks also have zeros for each of the padding entries. \n",
    "\n",
    "We used two parameters to the tokenizer to control the tokenization process. Some other useful [parameters](https://huggingface.co/docs/transformers/main_classes/tokenizer) include:\n",
    "\n",
    "* add_special_tokens (defaults to True) Whether or not to encode the sequences with the special tokens relative to their model.\n",
    "* padding (defaults to False) Activates and controls truncation.\n",
    "* max_length (optional) Controls the maximum length to use by one of the truncation/padding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "t81_558_class_11_02_tokenizers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
