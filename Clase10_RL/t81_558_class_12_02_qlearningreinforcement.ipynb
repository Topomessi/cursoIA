{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zA-E4NR2-NoY"
   },
   "source": [
    "# Part 12.2: Introduction to Q-Learning\n",
    "\n",
    "Q-Learning is a foundational technique upon which deep reinforcement learning is based.  Before we explore deep reinforcement learning, it is essential to understand Q-Learning.  Several components make up any Q-Learning system.\n",
    "\n",
    "* **Agent** - The agent is an entity that exists in an environment that takes actions to affect the state of the environment, to receive rewards.\n",
    "* **Environment** - The environment is the universe that the agent exists in.  The environment is always in a specific state that is changed by the actions of the agent.\n",
    "* **Actions** - Steps that can be performed by the agent to alter the environment \n",
    "* **Step** - A step occurs each time that the agent performs an action and potentially changes the environment state.\n",
    "* **Episode** - A chain of steps that ultimately culminates in the environment entering a terminal state.\n",
    "* **Epoch** - A training iteration of the agent that contains some number of episodes.\n",
    "* **Terminal State** -  A state in which further actions do not make sense.  In many environments, a terminal state occurs when the agent has one, lost, or the environment exceeding the maximum number of steps.\n",
    "\n",
    "Q-Learning works by building a table that suggests an action for every possible state.  This approach runs into several problems.  First, the environment is usually composed of several continuous numbers, resulting in an infinite number of states. Q-Learning handles continuous states by binning these numeric values into ranges. \n",
    "\n",
    "Additionally, Q-Learning primarily deals with discrete actions, such as pressing a joystick up or down.  Out of the box, Q-Learning does not deal with continuous inputs, such as a car's accelerator that can be in a range of positions from released to fully engaged. Researchers have come up with clever tricks to allow Q-Learning to accommodate continuous actions.\n",
    "\n",
    "In the next chapter, we will learn more about deep reinforcement learning. Deep neural networks can help to solve the problems of continuous environments and action spaces.  For now, we will apply regular Q-Learning to the Mountain Car problem from OpenAI Gym.\n",
    "\n",
    "### Introducing the Mountain Car\n",
    "\n",
    "This section will demonstrate how Q-Learning can create a solution to the mountain car gym environment.  The Mountain car is an environment where a car must climb a mountain.  Because gravity is stronger than the car's engine, even with full throttle, it cannot merely accelerate up the steep slope. The vehicle is situated in a valley and must learn to utilize potential energy by driving up the opposite hill before the car can make it to the goal at the top of the rightmost hill.\n",
    "\n",
    "First, it might be helpful to visualize the mountain car environment.  The following code shows this environment.  This code makes use of TF-Agents to perform this render. Usually, we use TF-Agents for the type of deep reinforcement learning that we will see in the next module.  However, for now, TF-Agents is just used to render the mountain care environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in d:\\userfiles\\anaconda\\envs\\ia\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in d:\\userfiles\\anaconda\\envs\\ia\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in d:\\userfiles\\anaconda\\envs\\ia\\lib\\site-packages (from gym) (1.19.5)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in d:\\userfiles\\anaconda\\envs\\ia\\lib\\site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: scipy in d:\\userfiles\\anaconda\\envs\\ia\\lib\\site-packages (from gym) (1.6.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in d:\\userfiles\\anaconda\\envs\\ia\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: future in d:\\userfiles\\anaconda\\envs\\ia\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "IcTY1gkD-X5r",
    "outputId": "564a04c3-f86b-42c0-f6f0-5519f807b243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q tf-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q pyvirtualdisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q PILLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "ngaU1pVi-mjb",
    "outputId": "141951c6-adfd-4f0d-e5ce-22a515a02a08"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400 at 0x1BB65071340>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tf_agents\n",
    "from tf_agents.environments import suite_gym\n",
    "import PIL.Image\n",
    "\n",
    "env_name = 'MountainCar-v0'\n",
    "env = suite_gym.load(env_name)\n",
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQBcs3yAMo_P"
   },
   "source": [
    "The mountain car environment provides the following discrete actions:\n",
    "\n",
    "* 0 - Apply left force\n",
    "* 1 - Apply no force\n",
    "* 2 - Apply right force\n",
    "\n",
    "The mountain car environment is made up of the following continuous values:\n",
    "\n",
    "* state[0] - Position \n",
    "* state[1] - Velocity\n",
    "\n",
    "The following code shows an agent that applies full throttle to climb the hill.  The cart is not strong enough.  It will need to use potential energy from the mountain behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QMLS0b-tbc_j"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iCRJdn0G-NoZ",
    "outputId": "37de6cbf-0fce-4cae-f88e-df646520c0dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: State=[-0.46963752  0.00060151], Reward=-1.0\n",
      "Step 2: State=[-0.46843895  0.00119856], Reward=-1.0\n",
      "Step 3: State=[-0.4666522   0.00178675], Reward=-1.0\n",
      "Step 4: State=[-0.46429047  0.00236173], Reward=-1.0\n",
      "Step 5: State=[-0.46137121  0.00291926], Reward=-1.0\n",
      "Step 6: State=[-0.45791596  0.00345526], Reward=-1.0\n",
      "Step 7: State=[-0.45395014  0.00396582], Reward=-1.0\n",
      "Step 8: State=[-0.44950289  0.00444725], Reward=-1.0\n",
      "Step 9: State=[-0.4446068   0.00489609], Reward=-1.0\n",
      "Step 10: State=[-0.43929762  0.00530918], Reward=-1.0\n",
      "Step 11: State=[-0.43361398  0.00568364], Reward=-1.0\n",
      "Step 12: State=[-0.42759706  0.00601692], Reward=-1.0\n",
      "Step 13: State=[-0.42129024  0.00630682], Reward=-1.0\n",
      "Step 14: State=[-0.41473874  0.0065515 ], Reward=-1.0\n",
      "Step 15: State=[-0.40798925  0.00674948], Reward=-1.0\n",
      "Step 16: State=[-0.40108956  0.00689969], Reward=-1.0\n",
      "Step 17: State=[-0.39408814  0.00700142], Reward=-1.0\n",
      "Step 18: State=[-0.3870338   0.00705434], Reward=-1.0\n",
      "Step 19: State=[-0.37997528  0.00705852], Reward=-1.0\n",
      "Step 20: State=[-0.37296091  0.00701436], Reward=-1.0\n",
      "Step 21: State=[-0.36603827  0.00692265], Reward=-1.0\n",
      "Step 22: State=[-0.35925381  0.00678446], Reward=-1.0\n",
      "Step 23: State=[-0.35265261  0.0066012 ], Reward=-1.0\n",
      "Step 24: State=[-0.34627804  0.00637457], Reward=-1.0\n",
      "Step 25: State=[-0.34017153  0.00610651], Reward=-1.0\n",
      "Step 26: State=[-0.33437234  0.00579919], Reward=-1.0\n",
      "Step 27: State=[-0.32891734  0.005455  ], Reward=-1.0\n",
      "Step 28: State=[-0.32384085  0.00507649], Reward=-1.0\n",
      "Step 29: State=[-0.31917447  0.00466638], Reward=-1.0\n",
      "Step 30: State=[-0.31494695  0.00422752], Reward=-1.0\n",
      "Step 31: State=[-0.31118411  0.00376284], Reward=-1.0\n",
      "Step 32: State=[-0.30790873  0.00327538], Reward=-1.0\n",
      "Step 33: State=[-0.30514048  0.00276825], Reward=-1.0\n",
      "Step 34: State=[-0.30289587  0.00224461], Reward=-1.0\n",
      "Step 35: State=[-0.30118821  0.00170766], Reward=-1.0\n",
      "Step 36: State=[-0.30002759  0.00116062], Reward=-1.0\n",
      "Step 37: State=[-0.29942083  0.00060676], Reward=-1.0\n",
      "Step 38: State=[-2.99371493e-01  4.93351003e-05], Reward=-1.0\n",
      "Step 39: State=[-0.29987987 -0.00050838], Reward=-1.0\n",
      "Step 40: State=[-0.30094298 -0.00106311], Reward=-1.0\n",
      "Step 41: State=[-0.30255457 -0.00161159], Reward=-1.0\n",
      "Step 42: State=[-0.30470513 -0.00215056], Reward=-1.0\n",
      "Step 43: State=[-0.30738192 -0.00267679], Reward=-1.0\n",
      "Step 44: State=[-0.31056899 -0.00318707], Reward=-1.0\n",
      "Step 45: State=[-0.31424722 -0.00367823], Reward=-1.0\n",
      "Step 46: State=[-0.31839438 -0.00414716], Reward=-1.0\n",
      "Step 47: State=[-0.32298518 -0.00459081], Reward=-1.0\n",
      "Step 48: State=[-0.32799139 -0.00500621], Reward=-1.0\n",
      "Step 49: State=[-0.3333819 -0.0053905], Reward=-1.0\n",
      "Step 50: State=[-0.33912285 -0.00574095], Reward=-1.0\n",
      "Step 51: State=[-0.34517782 -0.00605497], Reward=-1.0\n",
      "Step 52: State=[-0.35150795 -0.00633014], Reward=-1.0\n",
      "Step 53: State=[-0.3580722  -0.00656424], Reward=-1.0\n",
      "Step 54: State=[-0.36482749 -0.00675529], Reward=-1.0\n",
      "Step 55: State=[-0.37172905 -0.00690156], Reward=-1.0\n",
      "Step 56: State=[-0.37873063 -0.00700158], Reward=-1.0\n",
      "Step 57: State=[-0.38578485 -0.00705421], Reward=-1.0\n",
      "Step 58: State=[-0.39284347 -0.00705862], Reward=-1.0\n",
      "Step 59: State=[-0.3998578  -0.00701433], Reward=-1.0\n",
      "Step 60: State=[-0.40677902 -0.00692122], Reward=-1.0\n",
      "Step 61: State=[-0.41355856 -0.00677954], Reward=-1.0\n",
      "Step 62: State=[-0.42014849 -0.00658993], Reward=-1.0\n",
      "Step 63: State=[-0.42650191 -0.00635342], Reward=-1.0\n",
      "Step 64: State=[-0.43257329 -0.00607139], Reward=-1.0\n",
      "Step 65: State=[-0.43831892 -0.00574563], Reward=-1.0\n",
      "Step 66: State=[-0.44369719 -0.00537827], Reward=-1.0\n",
      "Step 67: State=[-0.448669   -0.00497181], Reward=-1.0\n",
      "Step 68: State=[-0.45319806 -0.00452906], Reward=-1.0\n",
      "Step 69: State=[-0.45725121 -0.00405315], Reward=-1.0\n",
      "Step 70: State=[-0.46079869 -0.00354748], Reward=-1.0\n",
      "Step 71: State=[-0.46381439 -0.0030157 ], Reward=-1.0\n",
      "Step 72: State=[-0.46627607 -0.00246168], Reward=-1.0\n",
      "Step 73: State=[-0.46816555 -0.00188948], Reward=-1.0\n",
      "Step 74: State=[-0.46946887 -0.00130332], Reward=-1.0\n",
      "Step 75: State=[-0.47017639 -0.00070751], Reward=-1.0\n",
      "Step 76: State=[-4.70282853e-01 -1.06467456e-04], Reward=-1.0\n",
      "Step 77: State=[-0.46978749  0.00049537], Reward=-1.0\n",
      "Step 78: State=[-0.46869395  0.00109353], Reward=-1.0\n",
      "Step 79: State=[-0.46701035  0.00168361], Reward=-1.0\n",
      "Step 80: State=[-0.46474912  0.00226123], Reward=-1.0\n",
      "Step 81: State=[-0.46192698  0.00282214], Reward=-1.0\n",
      "Step 82: State=[-0.45856474  0.00336224], Reward=-1.0\n",
      "Step 83: State=[-0.45468716  0.00387757], Reward=-1.0\n",
      "Step 84: State=[-0.45032275  0.00436441], Reward=-1.0\n",
      "Step 85: State=[-0.4455035   0.00481926], Reward=-1.0\n",
      "Step 86: State=[-0.44026461  0.00523888], Reward=-1.0\n",
      "Step 87: State=[-0.43464424  0.00562037], Reward=-1.0\n",
      "Step 88: State=[-0.42868314  0.0059611 ], Reward=-1.0\n",
      "Step 89: State=[-0.42242433  0.00625881], Reward=-1.0\n",
      "Step 90: State=[-0.41591273  0.0065116 ], Reward=-1.0\n",
      "Step 91: State=[-0.40919479  0.00671793], Reward=-1.0\n",
      "Step 92: State=[-0.40231814  0.00687665], Reward=-1.0\n",
      "Step 93: State=[-0.39533116  0.00698698], Reward=-1.0\n",
      "Step 94: State=[-0.38828262  0.00704854], Reward=-1.0\n",
      "Step 95: State=[-0.38122131  0.00706131], Reward=-1.0\n",
      "Step 96: State=[-0.37419565  0.00702566], Reward=-1.0\n",
      "Step 97: State=[-0.36725337  0.00694228], Reward=-1.0\n",
      "Step 98: State=[-0.36044116  0.00681221], Reward=-1.0\n",
      "Step 99: State=[-0.35380435  0.00663681], Reward=-1.0\n",
      "Step 100: State=[-0.34738664  0.00641771], Reward=-1.0\n",
      "Step 101: State=[-0.34122982  0.00615682], Reward=-1.0\n",
      "Step 102: State=[-0.33537355  0.00585627], Reward=-1.0\n",
      "Step 103: State=[-0.32985513  0.00551842], Reward=-1.0\n",
      "Step 104: State=[-0.32470934  0.00514579], Reward=-1.0\n",
      "Step 105: State=[-0.31996828  0.00474106], Reward=-1.0\n",
      "Step 106: State=[-0.31566121  0.00430707], Reward=-1.0\n",
      "Step 107: State=[-0.31181448  0.00384673], Reward=-1.0\n",
      "Step 108: State=[-0.3084514   0.00336308], Reward=-1.0\n",
      "Step 109: State=[-0.3055922  0.0028592], Reward=-1.0\n",
      "Step 110: State=[-0.30325396  0.00233825], Reward=-1.0\n",
      "Step 111: State=[-0.30145055  0.00180341], Reward=-1.0\n",
      "Step 112: State=[-0.30019262  0.00125792], Reward=-1.0\n",
      "Step 113: State=[-0.29948759  0.00070503], Reward=-1.0\n",
      "Step 114: State=[-2.99339597e-01  1.47996254e-04], Reward=-1.0\n",
      "Step 115: State=[-0.2997495  -0.00040991], Reward=-1.0\n",
      "Step 116: State=[-0.3007149 -0.0009654], Reward=-1.0\n",
      "Step 117: State=[-0.30223013 -0.00151522], Reward=-1.0\n",
      "Step 118: State=[-0.30428624 -0.00205611], Reward=-1.0\n",
      "Step 119: State=[-0.30687106 -0.00258483], Reward=-1.0\n",
      "Step 120: State=[-0.30996922 -0.00309816], Reward=-1.0\n",
      "Step 121: State=[-0.31356215 -0.00359293], Reward=-1.0\n",
      "Step 122: State=[-0.31762816 -0.00406601], Reward=-1.0\n",
      "Step 123: State=[-0.32214251 -0.00451435], Reward=-1.0\n",
      "Step 124: State=[-0.32707746 -0.00493495], Reward=-1.0\n",
      "Step 125: State=[-0.33240241 -0.00532495], Reward=-1.0\n",
      "Step 126: State=[-0.33808399 -0.00568158], Reward=-1.0\n",
      "Step 127: State=[-0.3440862  -0.00600221], Reward=-1.0\n",
      "Step 128: State=[-0.35037062 -0.00628442], Reward=-1.0\n",
      "Step 129: State=[-0.35689655 -0.00652593], Reward=-1.0\n",
      "Step 130: State=[-0.36362129 -0.00672473], Reward=-1.0\n",
      "Step 131: State=[-0.37050032 -0.00687903], Reward=-1.0\n",
      "Step 132: State=[-0.37748764 -0.00698732], Reward=-1.0\n",
      "Step 133: State=[-0.38453603 -0.0070484 ], Reward=-1.0\n",
      "Step 134: State=[-0.39159741 -0.00706138], Reward=-1.0\n",
      "Step 135: State=[-0.39862313 -0.00702571], Reward=-1.0\n",
      "Step 136: State=[-0.40556435 -0.00694123], Reward=-1.0\n",
      "Step 137: State=[-0.41237245 -0.0068081 ], Reward=-1.0\n",
      "Step 138: State=[-0.41899935 -0.0066269 ], Reward=-1.0\n",
      "Step 139: State=[-0.42539794 -0.00639859], Reward=-1.0\n",
      "Step 140: State=[-0.43152242 -0.00612448], Reward=-1.0\n",
      "Step 141: State=[-0.43732873 -0.00580631], Reward=-1.0\n",
      "Step 142: State=[-0.44277487 -0.00544614], Reward=-1.0\n",
      "Step 143: State=[-0.44782126 -0.00504639], Reward=-1.0\n",
      "Step 144: State=[-0.4524311  -0.00460984], Reward=-1.0\n",
      "Step 145: State=[-0.45657065 -0.00413955], Reward=-1.0\n",
      "Step 146: State=[-0.46020954 -0.00363888], Reward=-1.0\n",
      "Step 147: State=[-0.46332098 -0.00311144], Reward=-1.0\n",
      "Step 148: State=[-0.46588204 -0.00256107], Reward=-1.0\n",
      "Step 149: State=[-0.46787382 -0.00199178], Reward=-1.0\n",
      "Step 150: State=[-0.4692816  -0.00140777], Reward=-1.0\n",
      "Step 151: State=[-0.47009495 -0.00081335], Reward=-1.0\n",
      "Step 152: State=[-4.70307860e-01 -2.12910258e-04], Reward=-1.0\n",
      "Step 153: State=[-4.69918751e-01  3.89108295e-04], Reward=-1.0\n",
      "Step 154: State=[-0.46893051  0.00098825], Reward=-1.0\n",
      "Step 155: State=[-0.46735044  0.00158007], Reward=-1.0\n",
      "Step 156: State=[-0.46519023  0.00216021], Reward=-1.0\n",
      "Step 157: State=[-0.46246585  0.00272438], Reward=-1.0\n",
      "Step 158: State=[-0.4591974   0.00326845], Reward=-1.0\n",
      "Step 159: State=[-0.45540896  0.00378844], Reward=-1.0\n",
      "Step 160: State=[-0.45112839  0.00428057], Reward=-1.0\n",
      "Step 161: State=[-0.44638707  0.00474132], Reward=-1.0\n",
      "Step 162: State=[-0.44121967  0.0051674 ], Reward=-1.0\n",
      "Step 163: State=[-0.43566385  0.00555582], Reward=-1.0\n",
      "Step 164: State=[-0.42975992  0.00590393], Reward=-1.0\n",
      "Step 165: State=[-0.42355052  0.0062094 ], Reward=-1.0\n",
      "Step 166: State=[-0.41708026  0.00647025], Reward=-1.0\n",
      "Step 167: State=[-0.41039537  0.00668489], Reward=-1.0\n",
      "Step 168: State=[-0.40354328  0.00685209], Reward=-1.0\n",
      "Step 169: State=[-0.39657226  0.00697102], Reward=-1.0\n",
      "Step 170: State=[-0.38953104  0.00704121], Reward=-1.0\n",
      "Step 171: State=[-0.38246845  0.0070626 ], Reward=-1.0\n",
      "Step 172: State=[-0.37543299  0.00703546], Reward=-1.0\n",
      "Step 173: State=[-0.36847254  0.00696045], Reward=-1.0\n",
      "Step 174: State=[-0.36163399  0.00683855], Reward=-1.0\n",
      "Step 175: State=[-0.35496295  0.00667105], Reward=-1.0\n",
      "Step 176: State=[-0.3485034   0.00645954], Reward=-1.0\n",
      "Step 177: State=[-0.34229751  0.00620589], Reward=-1.0\n",
      "Step 178: State=[-0.33638532  0.00591219], Reward=-1.0\n",
      "Step 179: State=[-0.33080457  0.00558075], Reward=-1.0\n",
      "Step 180: State=[-0.32559049  0.00521408], Reward=-1.0\n",
      "Step 181: State=[-0.32077566  0.00481483], Reward=-1.0\n",
      "Step 182: State=[-0.31638987  0.00438579], Reward=-1.0\n",
      "Step 183: State=[-0.31245997  0.0039299 ], Reward=-1.0\n",
      "Step 184: State=[-0.30900983  0.00345014], Reward=-1.0\n",
      "Step 185: State=[-0.30606021  0.00294961], Reward=-1.0\n",
      "Step 186: State=[-0.30362877  0.00243145], Reward=-1.0\n",
      "Step 187: State=[-0.30172994  0.00189883], Reward=-1.0\n",
      "Step 188: State=[-0.30037495  0.00135499], Reward=-1.0\n",
      "Step 189: State=[-0.29957178  0.00080317], Reward=-1.0\n",
      "Step 190: State=[-2.99325146e-01  2.46630267e-04], Reward=-1.0\n",
      "Step 191: State=[-0.2996365  -0.00031136], Reward=-1.0\n",
      "Step 192: State=[-0.30050402 -0.00086752], Reward=-1.0\n",
      "Step 193: State=[-0.3019226  -0.00141858], Reward=-1.0\n",
      "Step 194: State=[-0.30388388 -0.00196128], Reward=-1.0\n",
      "Step 195: State=[-0.30637626 -0.00249238], Reward=-1.0\n",
      "Step 196: State=[-0.30938493 -0.00300867], Reward=-1.0\n",
      "Step 197: State=[-0.31289188 -0.00350695], Reward=-1.0\n",
      "Step 198: State=[-0.31687597 -0.00398409], Reward=-1.0\n",
      "Step 199: State=[-0.32131299 -0.00443702], Reward=-1.0\n",
      "Step 200: State=[-0.32617573 -0.00486274], Reward=-1.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "i = 0\n",
    "while not done:\n",
    "    i += 1\n",
    "    state, reward, done, _ = env.step(2)\n",
    "    env.render()\n",
    "    print(f\"Step {i}: State={state}, Reward={reward}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IVcdwI-0-Nod"
   },
   "source": [
    "### Programmed Car\n",
    "\n",
    "Now we will look at a car that I hand-programmed.  This car is straightforward; however, it solves the problem. The programmed car always applies force to one direction or another.  It does not break.  Whatever direction the vehicle is currently rolling, the agent uses power in that direction.  Therefore, the car begins to climb a hill, is overpowered, and turns backward.  However, once it starts to roll backward force is immediately applied in this new direction.\n",
    "\n",
    "The following code implements this preprogrammed car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AMzYrqRy-Noe",
    "outputId": "7ae64efe-40cb-4576-9158-4674848a00c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: State=[-0.43034018 -0.00170261], Reward=-1.0\n",
      "Step 2: State=[-0.43373314 -0.00339296], Reward=-1.0\n",
      "Step 3: State=[-0.43879197 -0.00505882], Reward=-1.0\n",
      "Step 4: State=[-0.44548    -0.00668803], Reward=-1.0\n",
      "Step 5: State=[-0.45374858 -0.00826858], Reward=-1.0\n",
      "Step 6: State=[-0.4635372  -0.00978863], Reward=-1.0\n",
      "Step 7: State=[-0.47477386 -0.01123666], Reward=-1.0\n",
      "Step 8: State=[-0.48737539 -0.01260153], Reward=-1.0\n",
      "Step 9: State=[-0.50124807 -0.01387267], Reward=-1.0\n",
      "Step 10: State=[-0.51628825 -0.01504018], Reward=-1.0\n",
      "Step 11: State=[-0.53238325 -0.016095  ], Reward=-1.0\n",
      "Step 12: State=[-0.54941238 -0.01702913], Reward=-1.0\n",
      "Step 13: State=[-0.5672481  -0.01783572], Reward=-1.0\n",
      "Step 14: State=[-0.58575738 -0.01850928], Reward=-1.0\n",
      "Step 15: State=[-0.60480317 -0.01904579], Reward=-1.0\n",
      "Step 16: State=[-0.62424594 -0.01944277], Reward=-1.0\n",
      "Step 17: State=[-0.64394527 -0.01969933], Reward=-1.0\n",
      "Step 18: State=[-0.66376148 -0.01981621], Reward=-1.0\n",
      "Step 19: State=[-0.68355718 -0.0197957 ], Reward=-1.0\n",
      "Step 20: State=[-0.7031987  -0.01964153], Reward=-1.0\n",
      "Step 21: State=[-0.72255746 -0.01935876], Reward=-1.0\n",
      "Step 22: State=[-0.74151107 -0.01895361], Reward=-1.0\n",
      "Step 23: State=[-0.75994428 -0.01843321], Reward=-1.0\n",
      "Step 24: State=[-0.77774974 -0.01780546], Reward=-1.0\n",
      "Step 25: State=[-0.79482846 -0.01707871], Reward=-1.0\n",
      "Step 26: State=[-0.8110901  -0.01626165], Reward=-1.0\n",
      "Step 27: State=[-0.82645312 -0.01536301], Reward=-1.0\n",
      "Step 28: State=[-0.84084458 -0.01439146], Reward=-1.0\n",
      "Step 29: State=[-0.85419998 -0.0133554 ], Reward=-1.0\n",
      "Step 30: State=[-0.86646284 -0.01226286], Reward=-1.0\n",
      "Step 31: State=[-0.87758427 -0.01112143], Reward=-1.0\n",
      "Step 32: State=[-0.88752242 -0.00993815], Reward=-1.0\n",
      "Step 33: State=[-0.89624197 -0.00871954], Reward=-1.0\n",
      "Step 34: State=[-0.90371352 -0.00747155], Reward=-1.0\n",
      "Step 35: State=[-0.90991313 -0.00619961], Reward=-1.0\n",
      "Step 36: State=[-0.91482178 -0.00490866], Reward=-1.0\n",
      "Step 37: State=[-0.918425   -0.00360322], Reward=-1.0\n",
      "Step 38: State=[-0.92071246 -0.00228746], Reward=-1.0\n",
      "Step 39: State=[-0.92167775 -0.00096529], Reward=-1.0\n",
      "Step 40: State=[-9.21318209e-01  3.59544328e-04], Reward=-1.0\n",
      "Step 41: State=[-0.91763482  0.00368339], Reward=-1.0\n",
      "Step 42: State=[-0.91063791  0.00699691], Reward=-1.0\n",
      "Step 43: State=[-0.90034788  0.01029003], Reward=-1.0\n",
      "Step 44: State=[-0.88679656  0.01355132], Reward=-1.0\n",
      "Step 45: State=[-0.87002914  0.01676742], Reward=-1.0\n",
      "Step 46: State=[-0.85010661  0.01992253], Reward=-1.0\n",
      "Step 47: State=[-0.82710849  0.02299811], Reward=-1.0\n",
      "Step 48: State=[-0.80113581  0.02597268], Reward=-1.0\n",
      "Step 49: State=[-0.7723139   0.02882191], Reward=-1.0\n",
      "Step 50: State=[-0.74079495  0.03151894], Reward=-1.0\n",
      "Step 51: State=[-0.70675988  0.03403507], Reward=-1.0\n",
      "Step 52: State=[-0.6704192   0.03634069], Reward=-1.0\n",
      "Step 53: State=[-0.63201262  0.03840658], Reward=-1.0\n",
      "Step 54: State=[-0.5918072   0.04020542], Reward=-1.0\n",
      "Step 55: State=[-0.55009377  0.04171342], Reward=-1.0\n",
      "Step 56: State=[-0.50718185  0.04291193], Reward=-1.0\n",
      "Step 57: State=[-0.463393    0.04378885], Reward=-1.0\n",
      "Step 58: State=[-0.41905324  0.04433976], Reward=-1.0\n",
      "Step 59: State=[-0.37448478  0.04456846], Reward=-1.0\n",
      "Step 60: State=[-0.32999775  0.04448703], Reward=-1.0\n",
      "Step 61: State=[-0.28588246  0.04411529], Reward=-1.0\n",
      "Step 62: State=[-0.24240271  0.04347975], Reward=-1.0\n",
      "Step 63: State=[-0.19979055  0.04261216], Reward=-1.0\n",
      "Step 64: State=[-0.15824261  0.04154794], Reward=-1.0\n",
      "Step 65: State=[-0.11791822  0.04032439], Reward=-1.0\n",
      "Step 66: State=[-0.07893902  0.0389792 ], Reward=-1.0\n",
      "Step 67: State=[-0.04139005  0.03754897], Reward=-1.0\n",
      "Step 68: State=[-0.00532182  0.03606822], Reward=-1.0\n",
      "Step 69: State=[0.02924672 0.03456854], Reward=-1.0\n",
      "Step 70: State=[0.06232487 0.03307816], Reward=-1.0\n",
      "Step 71: State=[0.0939466  0.03162173], Reward=-1.0\n",
      "Step 72: State=[0.12416697 0.03022037], Reward=-1.0\n",
      "Step 73: State=[0.15305878 0.02889182], Reward=-1.0\n",
      "Step 74: State=[0.18070955 0.02765077], Reward=-1.0\n",
      "Step 75: State=[0.20721879 0.02650924], Reward=-1.0\n",
      "Step 76: State=[0.23269575 0.02547695], Reward=-1.0\n",
      "Step 77: State=[0.25725752 0.02456177], Reward=-1.0\n",
      "Step 78: State=[0.2810276  0.02377008], Reward=-1.0\n",
      "Step 79: State=[0.30413477 0.02310717], Reward=-1.0\n",
      "Step 80: State=[0.32671233 0.02257756], Reward=-1.0\n",
      "Step 81: State=[0.34889761 0.02218528], Reward=-1.0\n",
      "Step 82: State=[0.3708318  0.02193419], Reward=-1.0\n",
      "Step 83: State=[0.39265993 0.02182813], Reward=-1.0\n",
      "Step 84: State=[0.41453108 0.02187115], Reward=-1.0\n",
      "Step 85: State=[0.43659874 0.02206766], Reward=-1.0\n",
      "Step 86: State=[0.45902128 0.02242254], Reward=-1.0\n",
      "Step 87: State=[0.48196252 0.02294124], Reward=-1.0\n",
      "Step 88: State=[0.5055923  0.02362978], Reward=-1.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "  \n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "i = 0\n",
    "while not done:\n",
    "    i += 1\n",
    "    \n",
    "    if state[1]>0:\n",
    "        action = 2\n",
    "    else:\n",
    "        action = 0\n",
    "    \n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Step {i}: State={state}, Reward={reward}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDSKmI___tEl"
   },
   "source": [
    "We now visualize the preprogrammed car solving the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "_FyIrJokfDA6",
    "outputId": "64c4c788-2037-4831-9de5-e394608911da"
   },
   "outputs": [],
   "source": [
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAyqqoy9-Nog"
   },
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "Q-Learning is a system of rewards that the algorithm gives an agent for successfully moving the environment into a state considered successful.  These rewards are the Q-values from which this algorithm takes its name.  The final output from the Q-Learning algorithm is a table of Q-values that indicate the reward value of every action that the agent can take, given every possible environment state. The agent must bin continuous state values into a fixed finite number of columns.\n",
    "\n",
    "Learning occurs when the algorithm runs the agent and environment through a series of episodes and updates the Q-values based on the rewards received from actions taken; Figure 12.REINF provides a high-level overview of this reinforcement or Q-Learning loop.\n",
    "\n",
    "**Figure 12.REINF:Reinforcement/Q Learning**\n",
    "![Reinforcement Learning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/reinforcement.png \"Reinforcement Learning\")\n",
    "\n",
    "The Q-values can dictate action by selecting the action column with the highest Q-value for the current environment state.  The choice between choosing a random action and a Q-value driven action is governed by the epsilon ($\\epsilon$) parameter, which is the probability of random action.\n",
    "\n",
    "Each time through the training loop, the training algorithm updates the Q-values according to the following equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q^{new}(s_{t},a_{t}) \\leftarrow \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot  \\overbrace{\\bigg( \\underbrace{\\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}}}_{\\text{new value (temporal difference target)}} - \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} \\bigg) }^{\\text{temporal difference}}$\n",
    "\n",
    "There are several parameters in this equation:\n",
    "* alpha ($\\alpha$) - The learning rate, how much should the current step cause the Q-values to be updated.\n",
    "* lambda ($\\lambda$) - The discount factor is the percentage of future reward that the algorithm should consider in this update.\n",
    "\n",
    "This equation modifies several values:\n",
    "\n",
    "* $Q(s_t,a_t)$ - The Q-table.  For each combination of states, what reward would the agent likely receive for performing each action?\n",
    "* $s_t$ - The current state.\n",
    "* $r_t$ - The last reward received.\n",
    "* $a_t$ - The action that the agent will perform.\n",
    "\n",
    "The equation works by calculating a delta (temporal difference) that the equation should apply to the old state.  This learning rate ($\\alpha$) scales this delta.  A learning rate of 1.0 would fully implement the temporal difference to the Q-values each iteration and would likely be very chaotic.\n",
    "\n",
    "There are two parts to the temporal difference: the new and old values.  The new value is subtracted from the old value to provide a delta; the full amount that we would change the Q-value by if the learning rate did not scale this value.  The new value is a summation of the reward received from the last action and the maximum of the Q-values from the resulting state when the client takes this action. It is essential to add the maximum of action Q-values for the new state because it estimates the optimal future values from proceeding with this action. \n",
    "\n",
    "### Q-Learning Car\n",
    "\n",
    "We will now use Q-Learning to produce a car that learns to drive itself.  Look out, Tesla!  We begin by defining two essential functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrQgXqbS-Noi"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# This function converts the floating point state values into \n",
    "# discrete values. This is often called binning.  We divide \n",
    "# the range that the state values might occupy and assign \n",
    "# each region to a bucket.\n",
    "def calc_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low)/buckets\n",
    "    return tuple(discrete_state.astype(np.int))  \n",
    "\n",
    "# Run one game.  The q_table to use is provided.  We also \n",
    "# provide a flag to indicate if the game should be \n",
    "# rendered/animated.  Finally, we also provide\n",
    "# a flag to indicate if the q_table should be updated.\n",
    "def run_game(q_table, render, should_update):\n",
    "    done = False\n",
    "    discrete_state = calc_discrete_state(env.reset())\n",
    "    success = False\n",
    "    \n",
    "    while not done:\n",
    "        # Exploit or explore\n",
    "        if np.random.random() > epsilon:\n",
    "            # Exploit - use q-table to take current best action \n",
    "            # (and probably refine)\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Explore - t\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "            \n",
    "        # Run simulation step\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Convert continuous state to discrete\n",
    "        new_state_disc = calc_discrete_state(new_state)\n",
    "\n",
    "        # Have we reached the goal position (have we won?)?\n",
    "        if new_state[0] >= env.unwrapped.goal_position:\n",
    "            success = True\n",
    "          \n",
    "        # Update q-table\n",
    "        if should_update:\n",
    "            max_future_q = np.max(q_table[new_state_disc])\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * \\\n",
    "              (reward + DISCOUNT * max_future_q)\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_state_disc\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3j1M4MddIBF"
   },
   "source": [
    "Several hyperparameters are very important for Q-Learning. These parameters will likely need adjustment as you apply Q-Learning to other problems.  Because of this, it is crucial to understand the role of each parameter.\n",
    "\n",
    "* **LEARNING_RATE** The rate at which previous Q-values are updated based on new episodes run during training. \n",
    "* **DISCOUNT** The amount of significance to give estimates of future rewards when added to the reward for the current action taken.  A value of 0.95 would indicate a discount of 5% to the future reward estimates. \n",
    "* **EPISODES** The number of episodes to train over.  Increase this for more complex problems; however, training time also increases.\n",
    "* **SHOW_EVERY** How many episodes to allow to elapse before showing an update.\n",
    "* **DISCRETE_GRID_SIZE** How many buckets to use when converting each of the continuous state variables.  For example, [10, 10] indicates that the algorithm should use ten buckets for the first and second state variables.\n",
    "* **START_EPSILON_DECAYING** Epsilon is the probability that the agent will select a random action over what the Q-Table suggests. This value determines the starting probability of randomness.\n",
    "* **END_EPSILON_DECAYING** How many episodes should elapse before epsilon goes to zero and no random actions are permitted. For example, EPISODES//10  means only the first 1/10th of the episodes might have random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rI0TJc7T-Nol"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 50000\n",
    "SHOW_EVERY = 1000\n",
    "\n",
    "DISCRETE_GRID_SIZE = [10, 10]\n",
    "START_EPSILON_DECAYING = 0.5\n",
    "END_EPSILON_DECAYING = EPISODES//10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw3SjzSyAktT"
   },
   "source": [
    "We can now make the environment.  If we are running in Google COLAB then we wrap the environment to be displayed inside the web browser.  Next create the discrete buckets for state and build Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2IdL-46y-Noo"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "epsilon = 1  \n",
    "epsilon_change = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "buckets = (env.observation_space.high - env.observation_space.low) \\\n",
    "    /DISCRETE_GRID_SIZE\n",
    "q_table = np.random.uniform(low=-3, high=0, size=(DISCRETE_GRID_SIZE \\\n",
    "    + [env.action_space.n]))\n",
    "success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z2qG3ayDsRWz"
   },
   "source": [
    "We can now make the environment.  If we are running in Google COLAB then we wrap the environment to be displayed inside the web browser.  Next, create the discrete buckets for state and build Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "Ot7cF0bX-Nor",
    "outputId": "92e0504b-93ea-4c8c-f1b5-47b9cacb75f6"
   },
   "outputs": [],
   "source": [
    "episode = 0\n",
    "success_count = 0\n",
    "\n",
    "# Loop through the required number of episodes\n",
    "while episode<EPISODES:\n",
    "    episode+=1\n",
    "    done = False\n",
    "\n",
    "    # Run the game.  If we are local, display render animation at SHOW_EVERY\n",
    "    # intervals. \n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        #print(f\"Current episode: {episode}, success: {success_count}\" +\\\n",
    "        #      \" ({float(success_count)/SHOW_EVERY})\")\n",
    "        print(float(success_count)/SHOW_EVERY)\n",
    "        success = run_game(q_table, True, False)\n",
    "        success_count = 0\n",
    "    else:\n",
    "        success = run_game(q_table, False, True)\n",
    "        \n",
    "    # Count successes\n",
    "    if success:\n",
    "        success_count += 1\n",
    "\n",
    "    # Move epsilon towards its ending value, if it still needs to move\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon = max(0, epsilon - epsilon_change)\n",
    "\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDHTQkREFRSE"
   },
   "source": [
    "As you can see, the number of successful episodes generally increases as training progresses.  It is not advisable to stop the first time that we observe 100% success over 1,000 episodes. There is a randomness to most games, so it is not likely that an agent would retain its 100% success rate with a new run.  Once you observe that the agent has gotten 100% for several update intervals, it might be safe to stop training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMPR70U0sY7o"
   },
   "source": [
    "# Running and Observing the Agent\n",
    "\n",
    "Now that the algorithm has trained the agent, we can observe the agent in action. You can use the following code to see the agent in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "v_mf3A0h-Nox",
    "outputId": "38d34522-667b-4de3-e84d-b0311e88f1ca"
   },
   "outputs": [],
   "source": [
    "run_game(q_table, True, False)\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeQzNFSosdys"
   },
   "source": [
    "# Inspecting the Q-Table\n",
    "\n",
    "We can also display the Q-table.  The following code shows the action that the agent would perform for each environment state.  As the weights of a neural network, this table is not straightforward to interpret.  Some patterns do emerge in that directions do arise, as seen by calculating the means of rows and columns. The actions seem consistent at upper and lower halves of both velocity and position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "L62lyCTr-Noz",
    "outputId": "1d983119-83fa-490f-957e-8dd580b8425d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(q_table.argmax(axis=2))\n",
    "\n",
    "df.columns = [f'v-{x}' for x in range(DISCRETE_GRID_SIZE[0])]\n",
    "df.index = [f'p-{x}' for x in range(DISCRETE_GRID_SIZE[1])]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Z-ZOL-d5Jbon",
    "outputId": "77de2b61-b460-423f-e5e6-97bff7070d72"
   },
   "outputs": [],
   "source": [
    "df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "LOGA4HP9KHW3",
    "outputId": "388ec235-1926-4801-88f2-ff852239dc16"
   },
   "outputs": [],
   "source": [
    "df.mean(axis=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
