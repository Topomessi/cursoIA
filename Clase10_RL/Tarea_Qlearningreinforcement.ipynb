{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIOgB0oG-NoS"
   },
   "source": [
    "# Tarea sobre Q-learning reforzado\n",
    "\n",
    "Para esta tarea, utilizando como base el codigo del video del siguiente link: \n",
    "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_02_qlearningreinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Y para comprender el codigo, se tiene este link con la explicacion del mismo:\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=A3sYFcJY3lA&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Jkp14GlpzMh"
   },
   "source": [
    "Primeramente, se configura el Google Collab para que trabaje con la version correcta de TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7rHrnrVCmiz"
   },
   "source": [
    "# Part 12.2: Introduction to Q-Learning\n",
    "\n",
    "Q-Learning is a foundational technique upon which deep reinforcement learning is based.  Before we explore deep reinforcement learning, it is essential to understand Q-Learning.  Several components make up any Q-Learning system.\n",
    "\n",
    "* **Agent** - The agent is an entity that exists in an environment that takes actions to affect the state of the environment, to receive rewards.\n",
    "* **Environment** - The environment is the universe that the agent exists in.  The environment is always in a specific state that is changed by the actions of the agent.\n",
    "* **Actions** - Steps that can be performed by the agent to alter the environment \n",
    "* **Step** - A step occurs each time that the agent performs an action and potentially changes the environment state.\n",
    "* **Episode** - A chain of steps that ultimately culminates in the environment entering a terminal state.\n",
    "* **Epoch** - A training iteration of the agent that contains some number of episodes.\n",
    "* **Terminal State** -  A state in which further actions do not make sense.  In many environments, a terminal state occurs when the agent has one, lost, or the environment exceeding the maximum number of steps.\n",
    "\n",
    "Q-Learning works by building a table that suggests an action for every possible state.  This approach runs into several problems.  First, the environment is usually composed of several continuous numbers, resulting in an infinite number of states. Q-Learning handles continuous states by binning these numeric values into ranges. \n",
    "\n",
    "Additionally, Q-Learning primarily deals with discrete actions, such as pressing a joystick up or down.  Out of the box, Q-Learning does not deal with continuous inputs, such as a car's accelerator that can be in a range of positions from released to fully engaged. Researchers have come up with clever tricks to allow Q-Learning to accommodate continuous actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQBcs3yAMo_P"
   },
   "source": [
    "# Pendulo Invertido (Pendulum-v0)\n",
    "\n",
    "El pendulo invertido es un problema clasico en la literatura de control. En esta version del problema, el pendulo inicia en una posicion aleatoria, y el objetivo es girar el mismo para que se quede verticalmente hacia arriba.\n",
    "\n",
    "El source code del pendulo esta en este enlace:\n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py\n",
    "\n",
    "La wiki del pendulo esta disponible en el siguiente enlace:\n",
    "\n",
    "https://github.com/openai/gym/wiki/Pendulum-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "ngaU1pVi-mjb",
    "outputId": "6ff16df5-8210-4240-c085-8789350c4661"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x500 at 0x29867E89430>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tf_agents\n",
    "from tf_agents.environments import suite_gym\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "env_name = 'Pendulum-v0'\n",
    "env = suite_gym.load(env_name)\n",
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f_6E05urKYZ"
   },
   "source": [
    "# Controles:\n",
    "Espacio de accion (continuo)\n",
    "\n",
    "* action[0] $\\leftarrow$ Torque aplicado al pendulo. Rango: (-2.0,+2.0) \n",
    "\n",
    "The mountain car environment is made up of the following continuous values:\n",
    "\n",
    "* state[0] $\\leftarrow$ Angulo del pendulo\n",
    "* state[1] $\\leftarrow$ Velocidad del pendulo\n",
    "\n",
    "La funcion de recompensa por defecto depende del angulo del pendulo. Si el pendulo esta completamente vertical hacia arriba, el mismo recibe la recompensa maxima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVgvqTURrQ9m"
   },
   "source": [
    "Primeramente, se inicializa el codigo necesario para poder visualizar el movimiento del pendulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QMLS0b-tbc_j"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oiUBOhNVbKF"
   },
   "source": [
    "#Primera prueba\n",
    "Para la primera implementacion, el pendulo recibira un torque de 1.0 hasta llegar al objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCRJdn0G-NoZ",
    "outputId": "837f9f4e-c9d3-442c-91f5-d76d3b57d8d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: State=[-0.14600657  0.98928362  0.99750294], Reward=-2.7824148361995538\n",
      "Step 2: State=[-0.23867739  0.97109892  1.88946565], Reward=-3.049712187034412\n",
      "Step 3: State=[-0.37035683  0.92888956  2.76778984], Reward=-3.6406272024463675\n",
      "Step 4: State=[-0.53128447  0.84719349  3.61445701], Reward=-4.570305024379857\n",
      "Step 5: State=[-0.70335662  0.71083716  4.39985213], Reward=-5.848217260280256\n",
      "Step 6: State=[-0.8594831   0.51116415  5.08298   ], Reward=-7.463623844743724\n",
      "Step 7: State=[-0.96748116  0.2529431   5.61635311], Reward=-9.370974519607858\n",
      "Step 8: State=[-0.99911469 -0.04206953  5.95606044], Reward=-11.48359701133636\n",
      "Step 9: State=[-0.9408021  -0.33895636  6.07450829], Reward=-13.15543219581458\n",
      "Step 10: State=[-0.79950725 -0.60065644  5.97029102], Reward=-11.507380479973756\n",
      "Step 11: State=[-0.59958661 -0.80030987  5.66979869], Reward=-9.801798628604155\n",
      "Step 12: State=[-0.37278277 -0.92791864  5.21956628], Reward=-8.116487165667667\n",
      "Step 13: State=[-0.14778153 -0.98902003  4.6736273 ], Reward=-6.538824784642744\n",
      "Step 14: State=[ 0.05573941 -0.99844535  4.08186228], Reward=-5.140656659642787\n",
      "Step 15: State=[ 0.22789937 -0.97368469  3.48302827], Reward=-3.962469842335215\n",
      "Step 16: State=[ 0.36632647 -0.93048639  2.90276475], Reward=-3.0120986416630893\n",
      "Step 17: State=[ 0.47309718 -0.88101025  2.35489996], Reward=-2.2733945447962047\n",
      "Step 18: State=[ 0.55220778 -0.83370652  1.84414227], Reward=-1.7176250699999087\n",
      "Step 19: State=[ 0.60793183 -0.79398923  1.36886238], Reward=-1.3128605999649456\n",
      "Step 20: State=[ 0.64392832 -0.76508583  0.92337046], Reward=-1.0298968391056011\n",
      "Step 21: State=[ 0.66283563 -0.74876493  0.49955609], Reward=-0.8452064953657119\n",
      "Step 22: State=[ 0.66612312 -0.7458418   0.08798239], Reward=-0.7420046558039307\n",
      "Step 23: State=[ 0.65405198 -0.7564496  -0.32139896], Reward=-0.7103974272956933\n",
      "Step 24: State=[ 0.62567138 -0.78008674 -0.73873616], Reward=-0.7472665991050307\n",
      "Step 25: State=[ 0.57883706 -0.81544322 -1.17380121], Reward=-0.8562480930833102\n",
      "Step 26: State=[ 0.51029919 -0.85999694 -1.63538363], Reward=-1.047932687288205\n",
      "Step 27: State=[ 0.41597398 -0.90937652 -2.13038134], Reward=-1.3402188494461504\n",
      "Step 28: State=[ 0.29159406 -0.95654216 -2.66241373], Reward=-1.7585203058677568\n",
      "Step 29: State=[ 0.13399761 -0.99098166 -3.22982035], Reward=-2.33522344906494\n",
      "Step 30: State=[-0.0567205  -0.9983901  -3.82305659], Reward=-3.107402965224749\n",
      "Step 31: State=[-0.27428241 -0.96164919 -4.42184916], Reward=-4.111486316993389\n",
      "Step 32: State=[-0.50337277 -0.86406936 -4.99308605], Reward=-5.373743882104956\n",
      "Step 33: State=[-0.71878604 -0.69523135 -5.49113807], Reward=-6.896928714660626\n",
      "Step 34: State=[-0.88901192 -0.45788406 -5.86256158], Reward=-8.646681334948775\n",
      "Step 35: State=[-0.98510463 -0.171956   -6.05597462], Reward=-10.54540698238489\n",
      "Step 36: State=[-0.99169975  0.12857526 -6.03494162], Reward=-12.482124221691171\n",
      "Step 37: State=[-0.91375738  0.40626033 -5.78851018], Reward=-12.719175154812937\n",
      "Step 38: State=[-0.7743884   0.63271052 -5.33381493], Reward=-10.767693667908327\n",
      "Step 39: State=[-0.6054123   0.79591202 -4.70928204], Reward=-8.880567655936416\n",
      "Step 40: State=[-0.43691523  0.89950269 -3.96234802], Reward=-7.151930605425382\n",
      "Step 41: State=[-0.29100806  0.9567206  -3.13772101], Reward=-5.663398323375874\n",
      "Step 42: State=[-0.18077198  0.98352503 -2.27018056], Reward=-4.4677714233029056\n",
      "Step 43: State=[-0.1124064   0.99366232 -1.38253678], Reward=-3.587865291553728\n",
      "Step 44: State=[-0.08816535  0.99610585 -0.48729004], Reward=-3.0261137090534542\n",
      "Step 45: State=[-0.10855509  0.99409043  0.40978935], Reward=-2.7772793075288496\n",
      "Step 46: State=[-0.17316005  0.9848937   1.30535717], Reward=-2.83873387708504\n",
      "Step 47: State=[-0.27994677  0.96001552  2.19402745], Reward=-3.215840095807965\n",
      "Step 48: State=[-0.4231695   0.90605054  3.06403909], Reward=-3.9216756805166844\n",
      "Step 49: State=[-0.59045256  0.80707235  3.89357699], Reward=-4.970841199957101\n",
      "Step 50: State=[-0.7604875   0.64935257  4.64888125], Reward=-6.367629517965232\n",
      "Step 51: State=[-0.90371053  0.42814399  5.28589568], Reward=-8.090752221172593\n",
      "Step 52: State=[-0.98807508  0.15397285  5.75700367], Reward=-10.0805051012398\n",
      "Step 53: State=[-0.98928275 -0.14601247  6.02248332], Reward=-12.237506965105943\n",
      "Step 54: State=[-0.90058406 -0.4346819   6.06297396], Reward=-12.598392857987761\n",
      "Step 55: State=[-0.73574305 -0.67726078  5.88696253], Reward=-10.923331941256613\n",
      "Step 56: State=[-0.52295346 -0.85236124  5.52901694], Reward=-9.214924293780088\n",
      "Step 57: State=[-0.29391962 -0.95583014  5.03974602], Reward=-7.557104711917618\n",
      "Step 58: State=[-0.07461194 -0.99721264  4.47287341], Reward=-6.034518582759209\n",
      "Step 59: State=[ 0.11878572 -0.99291991  3.87496393], Reward=-4.709256423015784\n",
      "Step 60: State=[ 0.27931491 -0.96019955  3.28027399], Reward=-3.6100530136717413\n",
      "Step 61: State=[ 0.40646964 -0.91366429  2.71012433], Reward=-2.7352316659799385\n",
      "Step 62: State=[ 0.50322834 -0.86415348  2.17487611], Reward=-2.063064256464257\n",
      "Step 63: State=[ 0.57382493 -0.81897799  1.676761  ], Reward=-1.5628293896742904\n",
      "Step 64: State=[ 0.62239194 -0.78270573  1.21252751], Reward=-1.203038075681238\n",
      "Step 65: State=[ 0.65226586 -0.75799027  0.77549821], Reward=-0.9562256840872038\n",
      "Step 66: State=[ 0.66569157 -0.746227    0.35700551], Reward=-0.801129236965612\n",
      "Step 67: State=[ 0.66372427 -0.74797734 -0.05266474], Reward=-0.7233428586538727\n",
      "Step 68: State=[ 0.64620758 -0.76316169 -0.46364775], Reward=-0.7153182093711848\n",
      "Step 69: State=[ 0.61177583 -0.79103118 -0.88601902], Reward=-0.7762538679907389\n",
      "Step 70: State=[ 0.55788817 -0.82991613 -1.3292924 ], Reward=-0.9121459302329268\n",
      "Step 71: State=[ 0.48096278 -0.87674101 -1.8017295 ], Reward=-1.1360592988842382\n",
      "Step 72: State=[ 0.37675277 -0.92631385 -2.30928526], Reward=-1.4684775943590689\n",
      "Step 73: State=[ 0.24118549 -0.97047904 -2.85402065], Reward=-1.9373392255913435\n",
      "Step 74: State=[ 0.07193105 -0.99740961 -3.43187993], Reward=-2.577027405870428\n",
      "Step 75: State=[-0.12914195 -0.99162612 -4.02993713], Reward=-3.425190731702998\n",
      "Step 76: State=[-0.35291662 -0.93565478 -4.62365672], Reward=-4.516059289516667\n",
      "Step 77: State=[-0.58059262 -0.81419421 -5.1753978 ], Reward=-5.869445945527797\n",
      "Step 78: State=[-0.78410861 -0.62062363 -5.63604346], Reward=-7.476681079918598\n",
      "Step 79: State=[-0.93161598 -0.36344418 -5.95151118], Reward=-9.288553825245778\n",
      "Step 80: State=[-0.99767136 -0.06820453 -6.07409432], Reward=-11.213901203755428\n",
      "Step 81: State=[-0.97355117  0.22846907 -5.97524771], Reward=-13.135851039144356\n",
      "Step 82: State=[-0.87117804  0.49096723 -5.65389591], Reward=-12.045790617309619\n",
      "Step 83: State=[-0.71792237  0.69612317 -5.13567049], Reward=-10.106103607006588\n",
      "Step 84: State=[-0.54604353  0.8377568  -4.46357811], Reward=-8.263042591905291\n",
      "Step 85: State=[-0.38330431  0.92362211 -3.68526051], Reward=-6.609106825100784\n",
      "Step 86: State=[-0.24860911  0.9686039  -2.84254392], Reward=-5.217068603363798\n",
      "Step 87: State=[-0.15234394  0.98832754 -1.966091  ], Reward=-4.128836588982508\n",
      "Step 88: State=[-0.09903459  0.99508399 -1.07484534], Reward=-3.358816448720194\n",
      "Step 89: State=[-0.09014803  0.99592838 -0.17853235], Reward=-2.905407611001528\n",
      "Step 90: State=[-0.12585662  0.99204844  0.71841393], Reward=-2.7633306369951183\n",
      "Step 91: State=[-0.20534263  0.97869015  1.61245026], Reward=-2.9323787716305896\n",
      "Step 92: State=[-0.32559143  0.94551056  2.49646787], Reward=-3.4208972987468584\n",
      "Step 93: State=[-0.47891402  0.87786181  3.35560079], Reward=-4.243488431303351\n",
      "Step 94: State=[-0.6500248   0.75991299  4.16399715], Reward=-5.412789795013806\n",
      "Step 95: State=[-0.81446915  0.58020686  4.8839319 ], Reward=-6.9260548671982605\n",
      "Step 96: State=[-0.94089665  0.3386938   5.46908704], Reward=-8.749840281133961\n",
      "Step 97: State=[-0.9986549   0.05184967  5.8731074 ], Reward=-10.810067163533082\n",
      "Step 98: State=[-0.96860818 -0.24859243  6.06199465], Reward=-12.99670700076483\n",
      "Step 99: State=[-0.85121288 -0.52482058  6.02555033], Reward=-12.029992684546338\n",
      "Step 100: State=[-0.66626975 -0.74571081  5.78193489], Reward=-10.335104092729441\n",
      "Step 101: State=[-0.44445223 -0.89580256  5.37265178], Reward=-8.634038437515494\n",
      "Step 102: State=[-0.21629954 -0.97632705  4.85079986], Reward=-7.0139581199264915\n",
      "Step 103: State=[-0.00459494 -0.99998944  4.26855458], Reward=-5.553899386021474\n",
      "Step 104: State=[ 0.17788147 -0.98405192  3.66856249], Reward=-4.304913508948197\n",
      "Step 105: State=[ 0.32674676 -0.94511193  3.08052355], Reward=-3.284396508818831\n",
      "Step 106: State=[ 0.44300147 -0.89652088  2.52168961], Reward=-2.482450502313496\n",
      "Step 107: State=[ 0.53026134 -0.84783425  1.99929895], Reward=-1.873107828650638\n",
      "Step 108: State=[ 0.59283928 -0.8053208   1.51342326], Reward=-1.4246360455152662\n",
      "Step 109: State=[ 0.63464694 -0.77280222  1.05943266], Reward=-1.106546122991445\n",
      "Step 110: State=[ 0.65866498 -0.75243634  0.629831  ], Reward=-0.8933610550888393\n",
      "Step 111: State=[ 0.66673423 -0.74529555  0.21550375], Reward=-0.7661522323327871\n",
      "Step 112: State=[ 0.65949361 -0.75171017 -0.19346792], Reward=-0.7128882141172067\n",
      "Step 113: State=[ 0.63636933 -0.77138452 -0.60725055], Reward=-0.7283508126172836\n",
      "Step 114: State=[ 0.59558438 -0.80329275 -1.03578893], Reward=-0.8140608882348837\n",
      "Step 115: State=[ 0.53421597 -0.84534803 -1.4882585 ], Reward=-0.978408033079957\n",
      "Step 116: State=[ 0.44839289 -0.89383657 -1.97226952], Reward=-1.236976036781729\n",
      "Step 117: State=[ 0.33380213 -0.94264316 -2.49264695], Reward=-1.6128442286848668\n",
      "Step 118: State=[ 0.18674983 -0.9824075  -3.04962932], Reward=-2.1363659502460943\n",
      "Step 119: State=[ 0.00603094 -0.99998181 -3.63643495], Reward=-2.843556907909368\n",
      "Step 120: State=[-0.20434064 -0.97889984 -4.23642131], Reward=-3.7718565143866507\n",
      "Step 121: State=[-0.43209983 -0.90182578 -4.82059619], Reward=-4.951985842472658\n",
      "Step 122: State=[-0.65498908 -0.75563834 -5.34696553], Reward=-6.395589995333775\n",
      "Step 123: State=[-0.84273993 -0.53832092 -5.76369428], Reward=-8.08106680759342\n",
      "Step 124: State=[-0.96441576 -0.26439032 -6.01743497], Reward=-9.944114062172973\n",
      "Step 125: State=[-0.9993623   0.0357071  -6.06572771], Reward=-11.881948870453279\n",
      "Step 126: State=[-0.94598981  0.32419634 -5.88894738], Reward=-13.326783154086229\n",
      "Step 127: State=[-0.82252936  0.56872265 -5.49580013], Reward=-11.373112081317986\n",
      "Step 128: State=[-0.65929546  0.75188396 -4.91925813], Reward=-9.455927343380443\n",
      "Step 129: State=[-0.48784041  0.87293284 -4.20534516], Reward=-7.668114270614086\n",
      "Step 130: State=[-0.33309282  0.94289404 -3.40064554], Reward=-6.097600297334478\n",
      "Step 131: State=[-0.21081445  0.97752609 -2.54347501], Reward=-4.806983644856987\n",
      "Step 132: State=[-0.12903079  0.99164059 -1.66033043], Reward=-3.827744405614244\n",
      "Step 133: State=[-0.09093574  0.99585676 -0.76659999], Reward=-3.1673085013432885\n",
      "Step 134: State=[-0.09742141  0.99524322  0.13029258], Reward=-2.821539129255142\n",
      "Step 135: State=[-0.14836268  0.98893302  1.02672499], Reward=-2.7861644046893947\n",
      "Step 136: State=[-0.24239495  0.97017766  1.91842476], Reward=-3.0638145950779907\n",
      "Step 137: State=[-0.37522227  0.92693487  2.796058  ], Reward=-3.6655475965430377\n",
      "Step 138: State=[-0.53685043  0.84367743  3.64125915], Reward=-4.606511802126519\n",
      "Step 139: State=[-0.7088739   0.70533524  4.42401723], Reward=-5.895765296780845\n",
      "Step 140: State=[-0.86394484  0.50358644  5.10301865], Reward=-7.521642981979928\n",
      "Step 141: State=[-0.96984322  0.24372962  5.63070849], Reward=-9.437279405080245\n",
      "Step 142: State=[-0.99865009 -0.05194231  5.9635057 ], Reward=-11.554730873991053\n",
      "Step 143: State=[-0.93740533 -0.3482402   6.07454897], Reward=-13.103134784292774\n",
      "Step 144: State=[-0.79374089 -0.60825603  5.96336882], Reward=-11.452250614929094\n",
      "Step 145: State=[-0.5924333 -0.8056195  5.6571768], Reward=-9.745982652728172\n",
      "Step 146: State=[-0.36527463 -0.93089981  5.20296217], Reward=-8.062826491309973\n",
      "Step 147: State=[-0.14071982 -0.99004946  4.65478731], Reward=-6.490033450064494\n",
      "Step 148: State=[ 0.06188451 -0.99808332  4.06225022], Reward=-5.098596305951193\n",
      "Step 149: State=[ 0.23294847 -0.97248908  3.46368773], Reward=-3.9278831427481227\n",
      "Step 150: State=[ 0.37029308 -0.92891498  2.88432092], Reward=-2.9847753312568153\n",
      "Step 151: State=[ 0.47609276 -0.87939506  2.33763469], Reward=-2.2525358027509483\n",
      "Step 152: State=[ 0.55437404 -0.83226764  1.82808839], Reward=-1.7021974344137574\n",
      "Step 153: State=[ 0.60940113 -0.79286207  1.35388766], Reward=-1.3018447596783567\n",
      "Step 154: State=[ 0.64480421 -0.76434778  0.90924111], Reward=-1.0224254592748583\n",
      "Step 155: State=[ 0.66318493 -0.74845557  0.48598028], Reward=-0.8406227867167431\n",
      "Step 156: State=[ 0.66597349 -0.74597541  0.0746386 ], Reward=-0.7398772675439023\n",
      "Step 157: State=[ 0.65339151 -0.75702017 -0.33484296], Reward=-0.7105181922838973\n",
      "Step 158: State=[ 0.62444869 -0.78106583 -0.75260809], Reward=-0.7496471001655278\n",
      "Step 159: State=[ 0.57696271 -0.81677049 -1.18840746], Reward=-0.8611225588029522\n",
      "Step 160: State=[ 0.50765075 -0.86156295 -1.65098533], Reward=-1.0557680528156774\n",
      "Step 161: State=[ 0.41241004 -0.91099833 -2.14715754], Reward=-1.3517261853443647\n",
      "Step 162: State=[ 0.28698473 -0.95793516 -2.68040629], Reward=-1.7746532514379103\n",
      "Step 163: State=[ 0.12828033 -0.99173795 -3.24885766], Reward=-2.3571376373469253\n",
      "Step 164: State=[-0.06345561 -0.99798466 -3.84266112], Reward=-3.136337524651404\n",
      "Step 165: State=[-0.28169074 -0.95950525 -4.44114961], Reward=-4.14852328384388\n",
      "Step 166: State=[-0.51078245 -0.85971    -5.01077855], Reward=-5.419424034309986\n",
      "Step 167: State=[-0.72523299 -0.68850353 -5.50556105], Reward=-6.950779887089035\n",
      "Step 168: State=[-0.89345057 -0.44916153 -5.8719387 ], Reward=-8.706849680958\n",
      "Step 169: State=[-0.98676335 -0.16216684 -6.05880985], Reward=-10.608690070096829\n",
      "Step 170: State=[-0.99040547  0.13819192 -6.03043498], Reward=-12.544610018680805\n",
      "Step 171: State=[-0.91001539  0.41457446 -5.77679104], Reward=-12.655365771559856\n",
      "Step 172: State=[-0.7691616   0.63905432 -5.3158602 ], Reward=-10.704565377316117\n",
      "Step 173: State=[-0.59975885  0.8001808  -4.68656945], Reward=-8.821129861834036\n",
      "Step 174: State=[-0.43170128  0.90201663 -3.93643385], Reward=-7.099171772028861\n",
      "Step 175: State=[-0.28679723  0.95799131 -3.10992138], Reward=-5.619543478225057\n",
      "Step 176: State=[-0.17785922  0.98405594 -2.2414279 ], Reward=-4.4340070955055735\n",
      "Step 177: State=[-0.11091259  0.99383017 -1.35338594], Reward=-3.564524134810315\n",
      "Step 178: State=[-0.08812612  0.99610932 -0.45801332], Reward=-3.013079381625951\n",
      "Step 179: State=[-0.10997115  0.99393478  0.43906868], Reward=-2.774381110479547\n",
      "Step 180: State=[-0.1759985   0.98439043  1.33451976], Reward=-2.8460066886696844\n",
      "Step 181: State=[-0.28409331  0.95879664  2.22281259], Reward=-3.2336066995145165\n",
      "Step 182: State=[-0.42834111  0.90361712  3.09191007], Reward=-3.950438807187055\n",
      "Step 183: State=[-0.59610186  0.80290882  3.9196229 ], Reward=-5.010981639065122\n",
      "Step 184: State=[-0.7657634   0.64312239  4.67180452], Reward=-6.418941274868365\n",
      "Step 185: State=[-0.90755931  0.41992393  5.30414631], Reward=-8.151941236398986\n",
      "Step 186: State=[-0.98951933  0.14440044  5.76908926], Reward=-10.148912812880177\n",
      "Step 187: State=[-0.9877847  -0.15582485  6.02738958], Reward=-12.309363792513603\n",
      "Step 188: State=[-0.89627946 -0.44348971  6.06052095], Reward=-12.544944875718427\n",
      "Step 189: State=[-0.72937824 -0.68411065  5.87790366], Reward=-10.867674142399006\n",
      "Step 190: State=[-0.51556903 -0.85684805  5.51482068], Reward=-9.159517265923602\n",
      "Step 191: State=[-0.28649108 -0.95808291  5.02218464], Reward=-7.50484551881714\n",
      "Step 192: State=[-0.06782914 -0.99769695  4.45362246], Reward=-5.987890108285954\n",
      "Step 193: State=[ 0.12456185 -0.99221185  3.85534975], Reward=-4.669739538754769\n",
      "Step 194: State=[ 0.28398317 -0.95882927  3.26119086], Reward=-3.5780281855713603\n",
      "Step 195: State=[ 0.41008679 -0.91204651  2.69206891], Reward=-2.710242136743218\n",
      "Step 196: State=[ 0.50592234 -0.86257903  2.15803403], Reward=-2.044194933186631\n",
      "Step 197: State=[ 0.57573754 -0.81763457  1.66109976], Reward=-1.5490296615927488\n",
      "Step 198: State=[ 0.62364707 -0.78170604  1.19787384], Reward=-1.1933301940366787\n",
      "Step 199: State=[ 0.65295491 -0.75739678  0.76159431], Reward=-0.9498110643461674\n",
      "Step 200: State=[ 0.665868   -0.74606957  0.34354672], Reward=-0.7974283208544397\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "i = 0\n",
    "while not done:\n",
    "    i += 1\n",
    "    state, reward, done, _ = env.step([1.0])\n",
    "    env.render()\n",
    "    print(f\"Step {i}: State={state}, Reward={reward}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVcdwI-0-Nod"
   },
   "source": [
    "### Pendulo Programado\n",
    "\n",
    "\n",
    "Ahora programamos el pendulo para llegar al objetivo. Se le imponen ciertas reglas para indicar que torque a emplear. Inicialmente, voy a darle un torque de +2.0 para iniciar el movimiento. Seguido de ello, cada que el pendulo este en un angulo positivo y su velocidad llegue a cero, voy a darle un torque de -0.65 para impulsarlo en el sentido contrario. Analogamente, cuando el pendulo este en un angulo negativo y su velocidad llega a cero, le doy un torque de 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMzYrqRy-Noe",
    "outputId": "af94868a-692b-4613-a6f0-d0e7ad34b13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: State=[0.27268932 0.96210214 1.14915014], Reward=-1.549948971326095\n",
      "Step 3: State=[0.18163696 0.98336566 1.87072675], Reward=-1.8080674328955084\n",
      "Step 4: State=[0.05221454 0.99863589 2.60825099], Reward=-2.276909727109774\n",
      "Step 5: State=[-0.11536565  0.99332309  3.35722791], Reward=-2.9863158094402387\n",
      "Step 6: State=[-0.31526332  0.94900424  4.10222023], Reward=-3.9711083977113955\n",
      "Step 7: State=[-0.53239958  0.84649317  4.81397341], Reward=-5.260708470428522\n",
      "Step 8: State=[-0.74054093  0.67201126  5.44884329], Reward=-6.863834957432022\n",
      "Step 9: State=[-0.90505862  0.42528684  5.95285173], Reward=-8.751433232196547\n",
      "Step 10: State=[-0.99211166  0.12535729  6.27181686], Reward=-10.846144550533436\n",
      "Step 11: State=[-0.98150916 -0.19141518  6.36583483], Reward=-13.029249834098616\n",
      "Step 12: State=[-0.87579426 -0.4826846   6.22227344], Reward=-12.748920641186743\n",
      "Step 13: State=[-0.69904841 -0.71507435  5.86026   ], Reward=-10.830054219360584\n",
      "Step 14: State=[-0.48631558 -0.87378324  5.32395423], Reward=-8.932643431717155\n",
      "Step 15: State=[-0.27100535 -0.96257784  4.6686168 ], Reward=-7.155294898620252\n",
      "Step 16: State=[-0.07702677 -0.99702902  3.94668342], Reward=-5.584485479233996\n",
      "Step 17: State=[ 0.08274771 -0.99657053  3.19891165], Reward=-4.273203670158252\n",
      "Step 18: State=[ 0.20397505 -0.97897609  2.45148376], Reward=-3.237310350490966\n",
      "Step 19: State=[ 0.28717779 -0.9578773   1.71725169], Reward=-2.465238987821826\n",
      "Step 20: State=[ 0.33463832 -0.94234664  0.99884372], Reward=-1.9320593867938463\n",
      "Step 21: State=[ 0.34836435 -0.9373592   0.29208374], Reward=-1.6116233852016972\n",
      "Step 22: State=[ 0.32903246 -0.94431861 -0.41093567], Reward=-1.4846851968816044\n",
      "Step 23: State=[ 0.27570213 -0.96124312 -1.11917462], Reward=-1.543390369521851\n",
      "Step 24: State=[ 0.18622126 -0.98250783 -1.84010696], Reward=-1.7931661074464378\n",
      "Step 25: State=[ 0.05843203 -0.99829139 -2.57698784], Reward=-2.2526207168338845\n",
      "Step 26: State=[-0.10761121 -0.99419305 -3.32570638], Reward=-2.951231678014164\n",
      "Step 27: State=[-0.30637975 -0.95190937 -4.07135117], Reward=-3.9237850311482845\n",
      "Step 28: State=[-0.52324272 -0.8521837  -4.7852832 ], Reward=-5.200206175315532\n",
      "Step 29: State=[-0.73242218 -0.68085075 -5.42442097], Reward=-6.790435293849335\n",
      "Step 30: State=[-0.89950084 -0.43691903 -5.93505904], Reward=-8.66729963477178\n",
      "Step 31: State=[-0.99035044 -0.13858571 -6.26274832], Reward=-10.755483465964403\n",
      "Step 32: State=[-0.98396856  0.178342   -6.3666876 ], Reward=-12.93756490531968\n",
      "Step 33: State=[-0.88188643  0.47146189 -6.2329311 ], Reward=-12.828641885308478\n",
      "Step 34: State=[-0.70744872  0.70676467 -5.87933468], Reward=-10.910861506002846\n",
      "Step 35: State=[-0.49550732  0.86860376 -5.34926118], Reward=-9.010589704819221\n",
      "Step 36: State=[-0.27974489  0.96007437 -4.69780836], Reward=-7.22627936365532\n",
      "Step 37: State=[-0.08453973  0.99642011 -3.97775258], Reward=-5.645460465197101\n",
      "Step 38: State=[ 0.07680541  0.9970461  -3.2304375 ], Reward=-4.322723487598392\n",
      "Step 39: State=[ 0.19966279  0.97986467 -2.48265292], Reward=-3.27535528601355\n",
      "Step 40: State=[ 0.2844201   0.95869975 -1.74775442], Reward=-2.492660815175568\n",
      "Step 41: State=[ 0.33333433  0.94280869 -1.0287296 ], Reward=-1.9500010752087622\n",
      "Step 42: State=[ 0.34845203  0.93732661 -0.32162309], Reward=-1.6210869428141386\n",
      "Step 43: State=[0.33051626 0.9438003  0.38137187], Reward=-1.4862707645047868\n",
      "Step 44: State=[0.27865123 0.96039236 1.0892221 ], Reward=-1.5371666847378105\n",
      "Step 45: State=[0.19073772 0.98164104 1.80951637], Reward=-1.7786328262221374\n",
      "Step 46: State=[0.06458127 0.99791245 2.54574715], Reward=-2.228752509738485\n",
      "Step 47: State=[-0.09991583  0.99499589  3.29418148], Reward=-2.916631160610683\n",
      "Step 48: State=[-0.29752981  0.95471253  4.0404284 ], Reward=-3.8770003102502084\n",
      "Step 49: State=[-0.51407371  0.85774601  4.7564628 ], Reward=-5.1402627271496995\n",
      "Step 50: State=[-0.72422926  0.68955927  5.39977231], Reward=-6.7175484308065565\n",
      "Step 51: State=[-0.89380666  0.44845251  5.91694176], Reward=-8.583545291266157\n",
      "Step 52: State=[-0.98841426  0.15178029  6.25328114], Reward=-10.66499029159492\n",
      "Step 53: State=[-0.98625582 -0.16522549  6.36711636], Reward=-12.845809481297145\n",
      "Step 54: State=[-0.88784931 -0.46013434  6.24319724], Reward=-12.908248228490907\n",
      "Step 55: State=[-0.7157832  -0.69832257  5.89809649], Reward=-10.991696589397728\n",
      "Step 56: State=[-0.50469383 -0.86329841  5.37435456], Reward=-9.088742243230715\n",
      "Step 57: State=[-0.28852239 -0.95747315  4.72688075], Reward=-7.297627845873434\n",
      "Step 58: State=[-0.09211368 -0.9957485   4.00877589], Reward=-5.706896247300216\n",
      "Step 59: State=[ 0.07079451 -0.99749092  3.26196452], Reward=-4.372733012627892\n",
      "Step 60: State=[ 0.1952831  -0.98074691  2.51384633], Reward=-3.3138688836662844\n",
      "Step 61: State=[ 0.28159926 -0.9595321   1.77828614], Reward=-2.520506021431384\n",
      "Step 62: State=[ 0.33197096 -0.94328961  1.05863706], Reward=-1.9683185869011357\n",
      "Step 63: State=[ 0.34848168 -0.93731559  0.35116986], Reward=-1.6308910272508528\n",
      "Step 64: State=[ 0.33194044 -0.94330035 -0.35181684], Reward=-1.4881817961034955\n",
      "Step 65: State=[ 0.28153679 -0.95955043 -1.0592921 ], Reward=-1.5312770288614348\n",
      "Step 66: State=[ 0.19518646 -0.98076615 -1.77895492], Reward=-1.7644657601442482\n",
      "Step 67: State=[ 0.07066215 -0.99750031 -2.51452954], Reward=-2.2053025966308537\n",
      "Step 68: State=[-0.09228018 -0.99573308 -3.26265477], Reward=-2.8825116190031936\n",
      "Step 69: State=[-0.28871498 -0.95741509 -4.00945458], Reward=-3.830752408129533\n",
      "Step 70: State=[-0.50489488 -0.86318084 -4.7275159 ], Reward=-5.08087839795483\n",
      "Step 71: State=[-0.71596491 -0.69813626 -5.37490153], Reward=-6.6451779903463954\n",
      "Step 72: State=[-0.88797839 -0.45988518 -5.89850373], Reward=-8.500177585042524\n",
      "Step 73: State=[-0.98630396 -0.16493786 -6.24341761], Reward=-10.574674924060076\n",
      "Step 74: State=[-0.98836992  0.15206877 -6.367121  ], Reward=-12.753993165961273\n",
      "Step 75: State=[-0.89368048  0.4487039  -6.25306943], Reward=-12.987735135452091\n",
      "Step 76: State=[-0.72404907  0.68974846 -5.9165415 ], Reward=-11.07255238496742\n",
      "Step 77: State=[-0.51387285  0.85786636 -5.39923015], Reward=-9.16709358483384\n",
      "Step 78: State=[-0.29733644  0.95477277 -4.75583038], Reward=-7.36933477801322\n",
      "Step 79: State=[-0.09974801  0.99501273 -4.0397508 ], Reward=-5.768790187221825\n",
      "Step 80: State=[ 0.06471514  0.99790378 -3.29349125], Reward=-4.423232210159049\n",
      "Step 81: State=[ 0.19083584  0.98162196 -2.54506342], Reward=-3.3528526915783248\n",
      "Step 82: State=[ 0.27871508  0.96037384 -1.80884695], Reward=-2.548776682532163\n",
      "Step 83: State=[ 0.33054808  0.94378915 -1.08856657], Reward=-1.9870137484689367\n",
      "Step 84: State=[ 0.3484533   0.93732614 -0.38072471], Reward=-1.6410367511385295\n",
      "Step 85: State=[0.33330512 0.94281902 0.3222699 ], Reward=-1.4904184603102306\n",
      "Step 86: State=[0.28435901 0.95871787 1.02938416], Reward=-1.5257205559429319\n",
      "Step 87: State=[0.19956762 0.97988406 1.74842257], Reward=-1.750663116004901\n",
      "Step 88: State=[0.07667454 0.99705617 2.48333561], Reward=-2.182268488230565\n",
      "Step 89: State=[-0.0847049   0.99640608  3.23112774], Reward=-2.848870404372718\n",
      "Step 90: State=[-0.27993667  0.96001847  3.9784323 ], Reward=-3.785039439661091\n",
      "Step 91: State=[-0.49570852  0.86848896  4.69844615], Reward=-5.022053343963471\n",
      "Step 92: State=[-0.70763191  0.70658126  5.34981287], Reward=-6.573327436377985\n",
      "Step 93: State=[-0.88201838  0.471215    5.87974881], Reward=-8.417203753380555\n",
      "Step 94: State=[-0.98402049  0.17805528  6.23316007], Reward=-10.484547203468665\n",
      "Step 95: State=[-0.99030992 -0.13887497  6.36670153], Reward=-12.662125639584914\n",
      "Step 96: State=[-0.89937762 -0.43717261  6.2625453 ], Reward=-13.067098203922026\n",
      "Step 97: State=[-0.73224359 -0.68104282  5.93466584], Reward=-11.153421878790109\n",
      "Step 98: State=[-0.52304209 -0.85230685  5.42388373], Reward=-9.245636232286584\n",
      "Step 99: State=[-0.30618563 -0.95197183  4.78465359], Reward=-7.441394481662029\n",
      "Step 100: State=[-0.10744209 -0.99421134  4.07067471], Reward=-5.831139522511256\n",
      "Step 101: State=[ 0.05856739 -0.99828346  3.32501621], Reward=-4.474220954125064\n",
      "Step 102: State=[ 0.18632087 -0.98248895  2.57630361], Reward=-3.392308213960156\n",
      "Step 103: State=[ 0.27576738 -0.9612244   1.8394369 ], Reward=-2.5774748702196297\n",
      "Step 104: State=[ 0.32906558 -0.94430707  1.1185186 ], Reward=-2.0060884082969554\n",
      "Step 105: State=[ 0.34836689 -0.93735826  0.4102883 ], Reward=-1.6515252630380737\n",
      "Step 106: State=[ 0.33461041 -0.94235655 -0.2927304 ], Reward=-1.49298096776399\n",
      "Step 107: State=[ 0.28711809 -0.9578952  -0.99949781], Reward=-1.5204964623070252\n",
      "Step 108: State=[ 0.20388135 -0.9789956  -1.71791921], Reward=-1.7372231368937534\n",
      "Step 109: State=[ 0.08261835 -0.99658126 -2.45216591], Reward=-2.15964771609803\n",
      "Step 110: State=[-0.0771906  -0.99701635 -3.19960186], Reward=-2.815704858936054\n",
      "Step 111: State=[-0.27119628 -0.96252407 -3.94736412], Reward=-3.7398594637854465\n",
      "Step 112: State=[-0.48651687 -0.87367118 -4.66925717], Reward=-4.963787607971824\n",
      "Step 113: State=[-0.69923301 -0.71489384 -5.32451056], Reward=-6.502000075641608\n",
      "Step 114: State=[-0.87592901 -0.48244002 -5.86068094], Reward=-8.334630885423309\n",
      "Step 115: State=[-0.98156484 -0.19112944 -6.22251096], Reward=-10.394616908291407\n",
      "Step 116: State=[-0.99207498  0.12564725 -6.36585804], Reward=-12.570216653403595\n",
      "Step 117: State=[-0.90493841  0.42554256 -6.2716226 ], Reward=-13.146333165383808\n",
      "Step 118: State=[-0.74036399  0.67220619 -5.95246568], Reward=-11.234298131620204\n",
      "Step 119: State=[-0.53219925  0.84661914 -5.44831104], Reward=-9.324362657307791\n",
      "Step 120: State=[-0.31506847  0.94906894 -4.81334669], Reward=-7.51380116920282\n",
      "Step 121: State=[-0.11519524  0.99334287 -4.10154498], Reward=-5.893941365981815\n",
      "Step 122: State=[ 0.0523514   0.99862873 -3.35653783], Reward=-4.525699025601416\n",
      "Step 123: State=[ 0.18173806  0.98334698 -2.60756628], Reward=-3.432236909183927\n",
      "Step 124: State=[ 0.27275597  0.96208325 -1.87005605], Reward=-2.606602650637386\n",
      "Step 125: State=[ 0.32752334  0.94484309 -1.14849361], Reward=-2.02554443573017\n",
      "Step 126: State=[ 0.34822245  0.93741193 -0.4398613 ], Reward=-1.6623577470370154\n",
      "Step 127: State=[0.33585642 0.94191319 0.26319765], Reward=-1.4958695709793715\n",
      "Step 128: State=[0.28981419 0.95708293 0.96963255], Reward=-1.5156039866541628\n",
      "Step 129: State=[0.2081278  0.97810164 1.68744474], Reward=-1.7241441026085964\n",
      "Step 130: State=[0.0884935  0.99607675 2.42102097], Reward=-2.137437833537577\n",
      "Step 131: State=[-0.06973787  0.99756535  3.16807854], Reward=-2.78301231754232\n",
      "Step 132: State=[-0.26249517  0.96493331  3.91625255], Reward=-3.6952104857908177\n",
      "Step 133: State=[-0.47732216  0.87872838  4.63995253], Reward=-4.906081121716317\n",
      "Step 134: State=[-0.69077094  0.72307365  5.29899881], Reward=-6.4311990587681915\n",
      "Step 135: State=[-0.8697127   0.49355833  5.84130405], Reward=-8.252465920269499\n",
      "Step 136: State=[-0.9789381   0.20415727  6.2114728 ], Reward=-10.304893750340257\n",
      "Step 137: State=[-0.99366431 -0.11238881  6.36459075], Reward=-12.478276024165114\n",
      "Step 138: State=[-0.91036059 -0.4138159   6.28029914], Reward=-13.225435885646762\n",
      "Step 139: State=[-0.74840751 -0.66323917  5.96993722], Reward=-11.31517428282094\n",
      "Step 140: State=[-0.54134197 -0.84080252  5.47250784], Reward=-9.403265305017587\n",
      "Step 141: State=[-0.32398348 -0.94606274  4.84190596], Reward=-7.58654894588001\n",
      "Step 142: State=[-0.12300678 -0.99240583  4.1323589 ], Reward=-5.957192705189636\n",
      "Step 143: State=[ 0.04606731 -0.99893834  3.38805453], Reward=-4.5776661109518475\n",
      "Step 144: State=[ 0.17708727 -0.98419515  2.63885078], Reward=-3.472640187849313\n",
      "Step 145: State=[ 0.26968065 -0.96294982  1.90070441], Reward=-2.636162082908533\n",
      "Step 146: State=[ 0.32592121 -0.94539693  1.17849205], Reward=-2.045383720225662\n",
      "Step 147: State=[ 0.34801996 -0.93748712  0.46944435], Reward=-1.673535422327593\n",
      "Step 148: State=[ 0.33704328 -0.94148916 -0.23367099], Reward=-1.499084564205783\n",
      "Step 149: State=[ 0.29244752 -0.95628157 -0.93978786], Reward=-1.5110424101513018\n",
      "Step 150: State=[ 0.21230713 -0.97720299 -1.65699904], Reward=-1.7114243300498955\n",
      "Step 151: State=[ 0.09429992 -0.99554383 -2.38990128], Reward=-2.1156364164753314\n",
      "Step 152: State=[-0.06234725 -0.99805452 -3.13655916], Reward=-2.750790109237934\n",
      "Step 153: State=[-0.25383467 -0.96724762 -3.88510005], Reward=-3.6510904595726417\n",
      "Step 154: State=[-0.46812657 -0.88366142 -4.61053576], Reward=-4.848933708269061\n",
      "Step 155: State=[-0.68224846 -0.7311204  -5.27328183], Reward=-6.360927381430201\n",
      "Step 156: State=[-0.86337191 -0.50456808 -5.82162213], Reward=-8.170715645211033\n",
      "Step 157: State=[-0.97614143 -0.21713571 -6.20004819], Reward=-10.215387369843544\n",
      "Step 158: State=[-0.9950772   0.09910285 -6.36289998], Reward=-12.386313628615891\n",
      "Step 159: State=[-0.91564193  0.40199484 -6.28857284], Reward=-13.304402365251784\n",
      "Step 160: State=[-0.75637139  0.65414243 -5.98707671], Reward=-11.396043554203821\n",
      "Step 161: State=[-0.55046788  0.83485634 -5.49646989], Reward=-9.482336598328331\n",
      "Step 162: State=[-0.3329291   0.94295186 -4.87032763], Reward=-7.659631811984077\n",
      "Step 163: State=[-0.13087597  0.99139875 -4.16311374], Reward=-6.020890402000837\n",
      "Step 164: State=[ 0.03971529  0.99921104 -3.41956467], Reward=-4.630121800001371\n",
      "Step 165: State=[ 0.1723684   0.98503255 -2.6701564 ], Reward=-3.5135194108361616\n",
      "Step 166: State=[ 0.26654124  0.96382352 -1.93138198], Reward=-2.6661552176880945\n",
      "Step 167: State=[ 0.32425907  0.94596832 -1.20851434], Reward=-2.065608170482982\n",
      "Step 168: State=[ 0.3477594   0.93758381 -0.49903811], Reward=-1.6850595427703983\n",
      "Step 169: State=[0.33817107 0.94108465 0.20414975], Reward=-1.5026262832766832\n",
      "Step 170: State=[0.29501826 0.95549162 0.90996324], Reward=-1.506811056512352\n",
      "Step 171: State=[0.21641948 0.97630047 1.62658195], Reward=-1.6990621735995628\n",
      "Step 172: State=[0.10003755 0.99498366 2.35880731], Reward=-2.094241064312461\n",
      "Step 173: State=[-0.05501929  0.99848529  3.10504505], Reward=-2.7190355588001687\n",
      "Step 174: State=[-0.24521605  0.96946846  3.85390902], Reward=-3.607497289869923\n",
      "Step 175: State=[-0.45893226  0.88847126  4.58101037], Reward=-4.79234508444987\n",
      "Step 176: State=[-0.67366828  0.73903386  5.24736381], Reward=-6.291187885580889\n",
      "Step 177: State=[-0.85690913  0.51546749  5.8016392 ], Reward=-8.089386694115056\n",
      "Step 178: State=[-0.97317603  0.23006174  6.18823982], Reward=-10.126107330622029\n",
      "Step 179: State=[-0.99631302 -0.08579259  6.36078613], Reward=-12.294339397931475\n",
      "Step 180: State=[-0.92078027 -0.39008165  6.29644169], Reward=-13.383228739709008\n",
      "Step 181: State=[-0.76425287 -0.6449167   6.00388046], Reward=-11.476899253768945\n",
      "Step 182: State=[-0.55957457 -0.82878001  5.52019293], Reward=-9.561568942380713\n",
      "Step 183: State=[-0.34190376 -0.93973497  4.89860792], Reward=-7.733043665154181\n",
      "Step 184: State=[-0.13880206 -0.99032014  4.19380669], Reward=-6.085031192249245\n",
      "Step 185: State=[ 0.03329551 -0.99944555  3.45106659], Reward=-4.683065584238453\n",
      "Step 186: State=[ 0.16758133 -0.98585825  2.70148242], Reward=-3.5548758873447426\n",
      "Step 187: State=[ 0.26333753 -0.96470376  1.96208873], Reward=-2.696584095690086\n",
      "Step 188: State=[ 0.32253677 -0.94655693  1.23856091], Reward=-2.0862197135524183\n",
      "Step 189: State=[ 0.34744074 -0.93770194  0.52864321], Reward=-1.6969313964431754\n",
      "Step 190: State=[ 0.3392399  -0.94069989 -0.17463324], Reward=-1.5064951054486735\n",
      "Step 191: State=[ 0.29752658 -0.95471353 -0.88015816], Reward=-1.5029092920684368\n",
      "Step 192: State=[ 0.22046502 -0.97539488 -1.59619331], Reward=-1.6870560254842502\n",
      "Step 193: State=[ 0.10570637 -0.99439739 -2.32773948], Reward=-2.073249400753988\n",
      "Step 194: State=[-0.04775449 -0.9988591  -3.07353752], Reward=-2.687745988239236\n",
      "Step 195: State=[-0.23664057 -0.97159726 -3.82268184], Reward=-3.564428834477357\n",
      "Step 196: State=[-0.44974136 -0.89315884 -4.55137979], Reward=-4.736314863252233\n",
      "Step 197: State=[-0.66503313 -0.74681385 -5.22124892], Reward=-6.22198326077844\n",
      "Step 198: State=[-0.85032689 -0.52625486 -5.78135931], Reward=-8.008485545949558\n",
      "Step 199: State=[-0.97004322 -0.24293239 -6.17605046], Reward=-10.0370631153701\n",
      "Step 200: State=[-0.99737123  0.07246127 -6.35824975], Reward=-12.202363312098505\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "  \n",
    "state = env.reset()\n",
    "done = False\n",
    "#Aqui le doy el torque inicial\n",
    "state, reward, done, _ = env.step([2.0])\n",
    "i = 1\n",
    "while not done:\n",
    "    i += 1\n",
    "    #Aqui impongo las condiciones para darle otro torque al pendulo\n",
    "    if (state[0]>0) and (state[1]==0):\n",
    "        action = -0.5\n",
    "    elif (state[0]<0) and (state[1]==0):\n",
    "        action= 0.5\n",
    "    else:\n",
    "        action =0\n",
    "    \n",
    "    state, reward, done, _ = env.step([action])\n",
    "    env.render()\n",
    "    print(f\"Step {i}: State={state}, Reward={reward}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDSKmI___tEl"
   },
   "source": [
    "We now visualize the preprogrammed car solving the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAyqqoy9-Nog"
   },
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "Q-Learning is a system of rewards that the algorithm gives an agent for successfully moving the environment into a state considered successful.  These rewards are the Q-values from which this algorithm takes its name.  The final output from the Q-Learning algorithm is a table of Q-values that indicate the reward value of every action that the agent can take, given every possible environment state. The agent must bin continuous state values into a fixed finite number of columns.\n",
    "\n",
    "Learning occurs when the algorithm runs the agent and environment through a series of episodes and updates the Q-values based on the rewards received from actions taken; Figure 12.REINF provides a high-level overview of this reinforcement or Q-Learning loop.\n",
    "\n",
    "**Figure 12.REINF:Reinforcement/Q Learning**\n",
    "![Reinforcement Learning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/reinforcement.png \"Reinforcement Learning\")\n",
    "\n",
    "The Q-values can dictate action by selecting the action column with the highest Q-value for the current environment state.  The choice between choosing a random action and a Q-value driven action is governed by the epsilon ($\\epsilon$) parameter, which is the probability of random action.\n",
    "\n",
    "Each time through the training loop, the training algorithm updates the Q-values according to the following equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z13r8S6hhh5g"
   },
   "source": [
    "$Q^{new}(s_{t},a_{t}) \\leftarrow \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot  \\overbrace{\\bigg( \\underbrace{\\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}}}_{\\text{new value (temporal difference target)}} - \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} \\bigg) }^{\\text{temporal difference}}$\n",
    "\n",
    "There are several parameters in this equation:\n",
    "* alpha ($\\alpha$) - The learning rate, how much should the current step cause the Q-values to be updated.\n",
    "* lambda ($\\lambda$) - The discount factor is the percentage of future reward that the algorithm should consider in this update.\n",
    "\n",
    "This equation modifies several values:\n",
    "\n",
    "* $Q(s_t,a_t)$ - The Q-table.  For each combination of states, what reward would the agent likely receive for performing each action?\n",
    "* $s_t$ - The current state.\n",
    "* $r_t$ - The last reward received.\n",
    "* $a_t$ - The action that the agent will perform.\n",
    "\n",
    "The equation works by calculating a delta (temporal difference) that the equation should apply to the old state.  This learning rate ($\\alpha$) scales this delta.  A learning rate of 1.0 would fully implement the temporal difference to the Q-values each iteration and would likely be very chaotic.\n",
    "\n",
    "There are two parts to the temporal difference: the new and old values.  The new value is subtracted from the old value to provide a delta; the full amount that we would change the Q-value by if the learning rate did not scale this value.  The new value is a summation of the reward received from the last action and the maximum of the Q-values from the resulting state when the client takes this action. It is essential to add the maximum of action Q-values for the new state because it estimates the optimal future values from proceeding with this action. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2iEOhvjM2a6"
   },
   "source": [
    "#Pendulo con Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "UrQgXqbS-Noi"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# This function converts the floating point state values into \n",
    "# discrete values. This is often called binning.  We divide \n",
    "# the range that the state values might occupy and assign \n",
    "# each region to a bucket.\n",
    "def calc_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low)/buckets\n",
    "    return tuple(discrete_state.astype(np.int))  \n",
    "\n",
    "# Run one game.  The q_table to use is provided.  We also \n",
    "# provide a flag to indicate if the game should be \n",
    "# rendered/animated.  Finally, we also provide\n",
    "# a flag to indicate if the q_table should be updated.\n",
    "def run_game(q_table, render, should_update):\n",
    "    done = False\n",
    "    discrete_state = calc_discrete_state(env.reset())\n",
    "    success = False\n",
    "    \n",
    "    while not done:\n",
    "        # Exploit or explore\n",
    "        if np.random.random() > epsilon:\n",
    "            # Exploit - use q-table to take current best action \n",
    "            # (and probably refine)\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Explore - t\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "            \n",
    "        # Run simulation step\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Convert continuous state to discrete\n",
    "        new_state_disc = calc_discrete_state(new_state)\n",
    "\n",
    "        # Have we reached the goal position (have we won?)?\n",
    "       # if new_state[0] >= env.unwrapped.goal_position:\n",
    "        #    success = True\n",
    "        if done:\n",
    "            success  = True  \n",
    "        # Update q-table\n",
    "        if should_update:\n",
    "            max_future_q = np.max(q_table[new_state_disc])\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * \\\n",
    "              (reward + DISCOUNT * max_future_q)\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_state_disc\n",
    "        \n",
    "        #if render:\n",
    "          # env.render()\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3j1M4MddIBF"
   },
   "source": [
    "Several hyperparameters are very important for Q-Learning. These parameters will likely need adjustment as you apply Q-Learning to other problems.  Because of this, it is crucial to understand the role of each parameter.\n",
    "\n",
    "* **LEARNING_RATE** The rate at which previous Q-values are updated based on new episodes run during training. \n",
    "* **DISCOUNT** The amount of significance to give estimates of future rewards when added to the reward for the current action taken.  A value of 0.95 would indicate a discount of 5% to the future reward estimates. \n",
    "* **EPISODES** The number of episodes to train over.  Increase this for more complex problems; however, training time also increases.\n",
    "* **SHOW_EVERY** How many episodes to allow to elapse before showing an update.\n",
    "* **DISCRETE_GRID_SIZE** How many buckets to use when converting each of the continuous state variables.  For example, [10, 10] indicates that the algorithm should use ten buckets for the first and second state variables.\n",
    "* **START_EPSILON_DECAYING** Epsilon is the probability that the agent will select a random action over what the Q-Table suggests. This value determines the starting probability of randomness.\n",
    "* **END_EPSILON_DECAYING** How many episodes should elapse before epsilon goes to zero and no random actions are permitted. For example, EPISODES//10  means only the first 1/10th of the episodes might have random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "rI0TJc7T-Nol"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 5000\n",
    "SHOW_EVERY = 10\n",
    "\n",
    "DISCRETE_GRID_SIZE = [20] * len(env.observation_space.high)\n",
    "START_EPSILON_DECAYING = 0.5\n",
    "END_EPSILON_DECAYING = EPISODES//10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw3SjzSyAktT"
   },
   "source": [
    "We can now make the environment.  If we are running in Google COLAB then we wrap the environment to be displayed inside the web browser.  Next create the discrete buckets for state and build Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "2IdL-46y-Noo",
    "outputId": "2a5aa399-d1ce-4135-c37d-0aac467ff400"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-c56f749ab164>:5: RuntimeWarning: overflow encountered in subtract\n",
      "  buckets = (env.observation_space.high - env.observation_space.low) \\\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "epsilon = 1  \n",
    "epsilon_change = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "buckets = (env.observation_space.high - env.observation_space.low) \\\n",
    "    /DISCRETE_GRID_SIZE\n",
    "q_table = np.random.uniform(low=-3, high=0, size=(DISCRETE_GRID_SIZE + [env.action_space.n]))\n",
    "success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2qG3ayDsRWz"
   },
   "source": [
    "We can now make the environment.  If we are running in Google COLAB then we wrap the environment to be displayed inside the web browser.  Next, create the discrete buckets for state and build Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "Ot7cF0bX-Nor",
    "outputId": "6242d074-cfcf-439d-890e-3ab84d8bfff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current episode: 10, success: 9 (0.9)\n",
      "Current episode: 20, success: 10 (1.0)\n",
      "Current episode: 30, success: 10 (1.0)\n",
      "Current episode: 40, success: 10 (1.0)\n",
      "Current episode: 50, success: 10 (1.0)\n",
      "Current episode: 60, success: 10 (1.0)\n",
      "Current episode: 70, success: 10 (1.0)\n",
      "Current episode: 80, success: 10 (1.0)\n",
      "Current episode: 90, success: 10 (1.0)\n",
      "Current episode: 100, success: 10 (1.0)\n",
      "Current episode: 110, success: 10 (1.0)\n",
      "Current episode: 120, success: 10 (1.0)\n",
      "Current episode: 130, success: 10 (1.0)\n",
      "Current episode: 140, success: 10 (1.0)\n",
      "Current episode: 150, success: 10 (1.0)\n",
      "Current episode: 160, success: 10 (1.0)\n",
      "Current episode: 170, success: 10 (1.0)\n",
      "Current episode: 180, success: 10 (1.0)\n",
      "Current episode: 190, success: 10 (1.0)\n",
      "Current episode: 200, success: 10 (1.0)\n",
      "Current episode: 210, success: 10 (1.0)\n",
      "Current episode: 220, success: 10 (1.0)\n",
      "Current episode: 230, success: 10 (1.0)\n",
      "Current episode: 240, success: 10 (1.0)\n",
      "Current episode: 250, success: 10 (1.0)\n",
      "Current episode: 260, success: 10 (1.0)\n",
      "Current episode: 270, success: 10 (1.0)\n",
      "Current episode: 280, success: 10 (1.0)\n",
      "Current episode: 290, success: 10 (1.0)\n",
      "Current episode: 300, success: 10 (1.0)\n",
      "Current episode: 310, success: 10 (1.0)\n",
      "Current episode: 320, success: 10 (1.0)\n",
      "Current episode: 330, success: 10 (1.0)\n",
      "Current episode: 340, success: 10 (1.0)\n",
      "Current episode: 350, success: 10 (1.0)\n",
      "Current episode: 360, success: 10 (1.0)\n",
      "Current episode: 370, success: 10 (1.0)\n",
      "Current episode: 380, success: 10 (1.0)\n",
      "Current episode: 390, success: 10 (1.0)\n",
      "Current episode: 400, success: 10 (1.0)\n",
      "Current episode: 410, success: 10 (1.0)\n",
      "Current episode: 420, success: 10 (1.0)\n",
      "Current episode: 430, success: 10 (1.0)\n",
      "Current episode: 440, success: 10 (1.0)\n",
      "Current episode: 450, success: 10 (1.0)\n",
      "Current episode: 460, success: 10 (1.0)\n",
      "Current episode: 470, success: 10 (1.0)\n",
      "Current episode: 480, success: 10 (1.0)\n",
      "Current episode: 490, success: 10 (1.0)\n",
      "Current episode: 500, success: 10 (1.0)\n",
      "Current episode: 510, success: 10 (1.0)\n",
      "Current episode: 520, success: 10 (1.0)\n",
      "Current episode: 530, success: 10 (1.0)\n",
      "Current episode: 540, success: 10 (1.0)\n",
      "Current episode: 550, success: 10 (1.0)\n",
      "Current episode: 560, success: 10 (1.0)\n",
      "Current episode: 570, success: 10 (1.0)\n",
      "Current episode: 580, success: 10 (1.0)\n",
      "Current episode: 590, success: 10 (1.0)\n",
      "Current episode: 600, success: 10 (1.0)\n",
      "Current episode: 610, success: 10 (1.0)\n",
      "Current episode: 620, success: 10 (1.0)\n",
      "Current episode: 630, success: 10 (1.0)\n",
      "Current episode: 640, success: 10 (1.0)\n",
      "Current episode: 650, success: 10 (1.0)\n",
      "Current episode: 660, success: 10 (1.0)\n",
      "Current episode: 670, success: 10 (1.0)\n",
      "Current episode: 680, success: 10 (1.0)\n",
      "Current episode: 690, success: 10 (1.0)\n",
      "Current episode: 700, success: 10 (1.0)\n",
      "Current episode: 710, success: 10 (1.0)\n",
      "Current episode: 720, success: 10 (1.0)\n",
      "Current episode: 730, success: 10 (1.0)\n",
      "Current episode: 740, success: 10 (1.0)\n",
      "Current episode: 750, success: 10 (1.0)\n",
      "Current episode: 760, success: 10 (1.0)\n",
      "Current episode: 770, success: 10 (1.0)\n",
      "Current episode: 780, success: 10 (1.0)\n",
      "Current episode: 790, success: 10 (1.0)\n",
      "Current episode: 800, success: 10 (1.0)\n",
      "Current episode: 810, success: 10 (1.0)\n",
      "Current episode: 820, success: 10 (1.0)\n",
      "Current episode: 830, success: 10 (1.0)\n",
      "Current episode: 840, success: 10 (1.0)\n",
      "Current episode: 850, success: 10 (1.0)\n",
      "Current episode: 860, success: 10 (1.0)\n",
      "Current episode: 870, success: 10 (1.0)\n",
      "Current episode: 880, success: 10 (1.0)\n",
      "Current episode: 890, success: 10 (1.0)\n",
      "Current episode: 900, success: 10 (1.0)\n",
      "Current episode: 910, success: 10 (1.0)\n",
      "Current episode: 920, success: 10 (1.0)\n",
      "Current episode: 930, success: 10 (1.0)\n",
      "Current episode: 940, success: 10 (1.0)\n",
      "Current episode: 950, success: 10 (1.0)\n",
      "Current episode: 960, success: 10 (1.0)\n",
      "Current episode: 970, success: 10 (1.0)\n",
      "Current episode: 980, success: 10 (1.0)\n",
      "Current episode: 990, success: 10 (1.0)\n",
      "Current episode: 1000, success: 10 (1.0)\n",
      "Current episode: 1010, success: 10 (1.0)\n",
      "Current episode: 1020, success: 10 (1.0)\n",
      "Current episode: 1030, success: 10 (1.0)\n",
      "Current episode: 1040, success: 10 (1.0)\n",
      "Current episode: 1050, success: 10 (1.0)\n",
      "Current episode: 1060, success: 10 (1.0)\n",
      "Current episode: 1070, success: 10 (1.0)\n",
      "Current episode: 1080, success: 10 (1.0)\n",
      "Current episode: 1090, success: 10 (1.0)\n",
      "Current episode: 1100, success: 10 (1.0)\n",
      "Current episode: 1110, success: 10 (1.0)\n",
      "Current episode: 1120, success: 10 (1.0)\n",
      "Current episode: 1130, success: 10 (1.0)\n",
      "Current episode: 1140, success: 10 (1.0)\n",
      "Current episode: 1150, success: 10 (1.0)\n",
      "Current episode: 1160, success: 10 (1.0)\n",
      "Current episode: 1170, success: 10 (1.0)\n",
      "Current episode: 1180, success: 10 (1.0)\n",
      "Current episode: 1190, success: 10 (1.0)\n",
      "Current episode: 1200, success: 10 (1.0)\n",
      "Current episode: 1210, success: 10 (1.0)\n",
      "Current episode: 1220, success: 10 (1.0)\n",
      "Current episode: 1230, success: 10 (1.0)\n",
      "Current episode: 1240, success: 10 (1.0)\n",
      "Current episode: 1250, success: 10 (1.0)\n",
      "Current episode: 1260, success: 10 (1.0)\n",
      "Current episode: 1270, success: 10 (1.0)\n",
      "Current episode: 1280, success: 10 (1.0)\n",
      "Current episode: 1290, success: 10 (1.0)\n",
      "Current episode: 1300, success: 10 (1.0)\n",
      "Current episode: 1310, success: 10 (1.0)\n",
      "Current episode: 1320, success: 10 (1.0)\n",
      "Current episode: 1330, success: 10 (1.0)\n",
      "Current episode: 1340, success: 10 (1.0)\n",
      "Current episode: 1350, success: 10 (1.0)\n",
      "Current episode: 1360, success: 10 (1.0)\n",
      "Current episode: 1370, success: 10 (1.0)\n",
      "Current episode: 1380, success: 10 (1.0)\n",
      "Current episode: 1390, success: 10 (1.0)\n",
      "Current episode: 1400, success: 10 (1.0)\n",
      "Current episode: 1410, success: 10 (1.0)\n",
      "Current episode: 1420, success: 10 (1.0)\n",
      "Current episode: 1430, success: 10 (1.0)\n",
      "Current episode: 1440, success: 10 (1.0)\n",
      "Current episode: 1450, success: 10 (1.0)\n",
      "Current episode: 1460, success: 10 (1.0)\n",
      "Current episode: 1470, success: 10 (1.0)\n",
      "Current episode: 1480, success: 10 (1.0)\n",
      "Current episode: 1490, success: 10 (1.0)\n",
      "Current episode: 1500, success: 10 (1.0)\n",
      "Current episode: 1510, success: 10 (1.0)\n",
      "Current episode: 1520, success: 10 (1.0)\n",
      "Current episode: 1530, success: 10 (1.0)\n",
      "Current episode: 1540, success: 10 (1.0)\n",
      "Current episode: 1550, success: 10 (1.0)\n",
      "Current episode: 1560, success: 10 (1.0)\n",
      "Current episode: 1570, success: 10 (1.0)\n",
      "Current episode: 1580, success: 10 (1.0)\n",
      "Current episode: 1590, success: 10 (1.0)\n",
      "Current episode: 1600, success: 10 (1.0)\n",
      "Current episode: 1610, success: 10 (1.0)\n",
      "Current episode: 1620, success: 10 (1.0)\n",
      "Current episode: 1630, success: 10 (1.0)\n",
      "Current episode: 1640, success: 10 (1.0)\n",
      "Current episode: 1650, success: 10 (1.0)\n",
      "Current episode: 1660, success: 10 (1.0)\n",
      "Current episode: 1670, success: 10 (1.0)\n",
      "Current episode: 1680, success: 10 (1.0)\n",
      "Current episode: 1690, success: 10 (1.0)\n",
      "Current episode: 1700, success: 10 (1.0)\n",
      "Current episode: 1710, success: 10 (1.0)\n",
      "Current episode: 1720, success: 10 (1.0)\n",
      "Current episode: 1730, success: 10 (1.0)\n",
      "Current episode: 1740, success: 10 (1.0)\n",
      "Current episode: 1750, success: 10 (1.0)\n",
      "Current episode: 1760, success: 10 (1.0)\n",
      "Current episode: 1770, success: 10 (1.0)\n",
      "Current episode: 1780, success: 10 (1.0)\n",
      "Current episode: 1790, success: 10 (1.0)\n",
      "Current episode: 1800, success: 10 (1.0)\n",
      "Current episode: 1810, success: 10 (1.0)\n",
      "Current episode: 1820, success: 10 (1.0)\n",
      "Current episode: 1830, success: 10 (1.0)\n",
      "Current episode: 1840, success: 10 (1.0)\n",
      "Current episode: 1850, success: 10 (1.0)\n",
      "Current episode: 1860, success: 10 (1.0)\n",
      "Current episode: 1870, success: 10 (1.0)\n",
      "Current episode: 1880, success: 10 (1.0)\n",
      "Current episode: 1890, success: 10 (1.0)\n",
      "Current episode: 1900, success: 10 (1.0)\n",
      "Current episode: 1910, success: 10 (1.0)\n",
      "Current episode: 1920, success: 10 (1.0)\n",
      "Current episode: 1930, success: 10 (1.0)\n",
      "Current episode: 1940, success: 10 (1.0)\n",
      "Current episode: 1950, success: 10 (1.0)\n",
      "Current episode: 1960, success: 10 (1.0)\n",
      "Current episode: 1970, success: 10 (1.0)\n",
      "Current episode: 1980, success: 10 (1.0)\n",
      "Current episode: 1990, success: 10 (1.0)\n",
      "Current episode: 2000, success: 10 (1.0)\n",
      "Current episode: 2010, success: 10 (1.0)\n",
      "Current episode: 2020, success: 10 (1.0)\n",
      "Current episode: 2030, success: 10 (1.0)\n",
      "Current episode: 2040, success: 10 (1.0)\n",
      "Current episode: 2050, success: 10 (1.0)\n",
      "Current episode: 2060, success: 10 (1.0)\n",
      "Current episode: 2070, success: 10 (1.0)\n",
      "Current episode: 2080, success: 10 (1.0)\n",
      "Current episode: 2090, success: 10 (1.0)\n",
      "Current episode: 2100, success: 10 (1.0)\n",
      "Current episode: 2110, success: 10 (1.0)\n",
      "Current episode: 2120, success: 10 (1.0)\n",
      "Current episode: 2130, success: 10 (1.0)\n",
      "Current episode: 2140, success: 10 (1.0)\n",
      "Current episode: 2150, success: 10 (1.0)\n",
      "Current episode: 2160, success: 10 (1.0)\n",
      "Current episode: 2170, success: 10 (1.0)\n",
      "Current episode: 2180, success: 10 (1.0)\n",
      "Current episode: 2190, success: 10 (1.0)\n",
      "Current episode: 2200, success: 10 (1.0)\n",
      "Current episode: 2210, success: 10 (1.0)\n",
      "Current episode: 2220, success: 10 (1.0)\n",
      "Current episode: 2230, success: 10 (1.0)\n",
      "Current episode: 2240, success: 10 (1.0)\n",
      "Current episode: 2250, success: 10 (1.0)\n",
      "Current episode: 2260, success: 10 (1.0)\n",
      "Current episode: 2270, success: 10 (1.0)\n",
      "Current episode: 2280, success: 10 (1.0)\n",
      "Current episode: 2290, success: 10 (1.0)\n",
      "Current episode: 2300, success: 10 (1.0)\n",
      "Current episode: 2310, success: 10 (1.0)\n",
      "Current episode: 2320, success: 10 (1.0)\n",
      "Current episode: 2330, success: 10 (1.0)\n",
      "Current episode: 2340, success: 10 (1.0)\n",
      "Current episode: 2350, success: 10 (1.0)\n",
      "Current episode: 2360, success: 10 (1.0)\n",
      "Current episode: 2370, success: 10 (1.0)\n",
      "Current episode: 2380, success: 10 (1.0)\n",
      "Current episode: 2390, success: 10 (1.0)\n",
      "Current episode: 2400, success: 10 (1.0)\n",
      "Current episode: 2410, success: 10 (1.0)\n",
      "Current episode: 2420, success: 10 (1.0)\n",
      "Current episode: 2430, success: 10 (1.0)\n",
      "Current episode: 2440, success: 10 (1.0)\n",
      "Current episode: 2450, success: 10 (1.0)\n",
      "Current episode: 2460, success: 10 (1.0)\n",
      "Current episode: 2470, success: 10 (1.0)\n",
      "Current episode: 2480, success: 10 (1.0)\n",
      "Current episode: 2490, success: 10 (1.0)\n",
      "Current episode: 2500, success: 10 (1.0)\n",
      "Current episode: 2510, success: 10 (1.0)\n",
      "Current episode: 2520, success: 10 (1.0)\n",
      "Current episode: 2530, success: 10 (1.0)\n",
      "Current episode: 2540, success: 10 (1.0)\n",
      "Current episode: 2550, success: 10 (1.0)\n",
      "Current episode: 2560, success: 10 (1.0)\n",
      "Current episode: 2570, success: 10 (1.0)\n",
      "Current episode: 2580, success: 10 (1.0)\n",
      "Current episode: 2590, success: 10 (1.0)\n",
      "Current episode: 2600, success: 10 (1.0)\n",
      "Current episode: 2610, success: 10 (1.0)\n",
      "Current episode: 2620, success: 10 (1.0)\n",
      "Current episode: 2630, success: 10 (1.0)\n",
      "Current episode: 2640, success: 10 (1.0)\n",
      "Current episode: 2650, success: 10 (1.0)\n",
      "Current episode: 2660, success: 10 (1.0)\n",
      "Current episode: 2670, success: 10 (1.0)\n",
      "Current episode: 2680, success: 10 (1.0)\n",
      "Current episode: 2690, success: 10 (1.0)\n",
      "Current episode: 2700, success: 10 (1.0)\n",
      "Current episode: 2710, success: 10 (1.0)\n",
      "Current episode: 2720, success: 10 (1.0)\n",
      "Current episode: 2730, success: 10 (1.0)\n",
      "Current episode: 2740, success: 10 (1.0)\n",
      "Current episode: 2750, success: 10 (1.0)\n",
      "Current episode: 2760, success: 10 (1.0)\n",
      "Current episode: 2770, success: 10 (1.0)\n",
      "Current episode: 2780, success: 10 (1.0)\n",
      "Current episode: 2790, success: 10 (1.0)\n",
      "Current episode: 2800, success: 10 (1.0)\n",
      "Current episode: 2810, success: 10 (1.0)\n",
      "Current episode: 2820, success: 10 (1.0)\n",
      "Current episode: 2830, success: 10 (1.0)\n",
      "Current episode: 2840, success: 10 (1.0)\n",
      "Current episode: 2850, success: 10 (1.0)\n",
      "Current episode: 2860, success: 10 (1.0)\n",
      "Current episode: 2870, success: 10 (1.0)\n",
      "Current episode: 2880, success: 10 (1.0)\n",
      "Current episode: 2890, success: 10 (1.0)\n",
      "Current episode: 2900, success: 10 (1.0)\n",
      "Current episode: 2910, success: 10 (1.0)\n",
      "Current episode: 2920, success: 10 (1.0)\n",
      "Current episode: 2930, success: 10 (1.0)\n",
      "Current episode: 2940, success: 10 (1.0)\n",
      "Current episode: 2950, success: 10 (1.0)\n",
      "Current episode: 2960, success: 10 (1.0)\n",
      "Current episode: 2970, success: 10 (1.0)\n",
      "Current episode: 2980, success: 10 (1.0)\n",
      "Current episode: 2990, success: 10 (1.0)\n",
      "Current episode: 3000, success: 10 (1.0)\n",
      "Current episode: 3010, success: 10 (1.0)\n",
      "Current episode: 3020, success: 10 (1.0)\n",
      "Current episode: 3030, success: 10 (1.0)\n",
      "Current episode: 3040, success: 10 (1.0)\n",
      "Current episode: 3050, success: 10 (1.0)\n",
      "Current episode: 3060, success: 10 (1.0)\n",
      "Current episode: 3070, success: 10 (1.0)\n",
      "Current episode: 3080, success: 10 (1.0)\n",
      "Current episode: 3090, success: 10 (1.0)\n",
      "Current episode: 3100, success: 10 (1.0)\n",
      "Current episode: 3110, success: 10 (1.0)\n",
      "Current episode: 3120, success: 10 (1.0)\n",
      "Current episode: 3130, success: 10 (1.0)\n",
      "Current episode: 3140, success: 10 (1.0)\n",
      "Current episode: 3150, success: 10 (1.0)\n",
      "Current episode: 3160, success: 10 (1.0)\n",
      "Current episode: 3170, success: 10 (1.0)\n",
      "Current episode: 3180, success: 10 (1.0)\n",
      "Current episode: 3190, success: 10 (1.0)\n",
      "Current episode: 3200, success: 10 (1.0)\n",
      "Current episode: 3210, success: 10 (1.0)\n",
      "Current episode: 3220, success: 10 (1.0)\n",
      "Current episode: 3230, success: 10 (1.0)\n",
      "Current episode: 3240, success: 10 (1.0)\n",
      "Current episode: 3250, success: 10 (1.0)\n",
      "Current episode: 3260, success: 10 (1.0)\n",
      "Current episode: 3270, success: 10 (1.0)\n",
      "Current episode: 3280, success: 10 (1.0)\n",
      "Current episode: 3290, success: 10 (1.0)\n",
      "Current episode: 3300, success: 10 (1.0)\n",
      "Current episode: 3310, success: 10 (1.0)\n",
      "Current episode: 3320, success: 10 (1.0)\n",
      "Current episode: 3330, success: 10 (1.0)\n",
      "Current episode: 3340, success: 10 (1.0)\n",
      "Current episode: 3350, success: 10 (1.0)\n",
      "Current episode: 3360, success: 10 (1.0)\n",
      "Current episode: 3370, success: 10 (1.0)\n",
      "Current episode: 3380, success: 10 (1.0)\n",
      "Current episode: 3390, success: 10 (1.0)\n",
      "Current episode: 3400, success: 10 (1.0)\n",
      "Current episode: 3410, success: 10 (1.0)\n",
      "Current episode: 3420, success: 10 (1.0)\n",
      "Current episode: 3430, success: 10 (1.0)\n",
      "Current episode: 3440, success: 10 (1.0)\n",
      "Current episode: 3450, success: 10 (1.0)\n",
      "Current episode: 3460, success: 10 (1.0)\n",
      "Current episode: 3470, success: 10 (1.0)\n",
      "Current episode: 3480, success: 10 (1.0)\n",
      "Current episode: 3490, success: 10 (1.0)\n",
      "Current episode: 3500, success: 10 (1.0)\n",
      "Current episode: 3510, success: 10 (1.0)\n",
      "Current episode: 3520, success: 10 (1.0)\n",
      "Current episode: 3530, success: 10 (1.0)\n",
      "Current episode: 3540, success: 10 (1.0)\n",
      "Current episode: 3550, success: 10 (1.0)\n",
      "Current episode: 3560, success: 10 (1.0)\n",
      "Current episode: 3570, success: 10 (1.0)\n",
      "Current episode: 3580, success: 10 (1.0)\n",
      "Current episode: 3590, success: 10 (1.0)\n",
      "Current episode: 3600, success: 10 (1.0)\n",
      "Current episode: 3610, success: 10 (1.0)\n",
      "Current episode: 3620, success: 10 (1.0)\n",
      "Current episode: 3630, success: 10 (1.0)\n",
      "Current episode: 3640, success: 10 (1.0)\n",
      "Current episode: 3650, success: 10 (1.0)\n",
      "Current episode: 3660, success: 10 (1.0)\n",
      "Current episode: 3670, success: 10 (1.0)\n",
      "Current episode: 3680, success: 10 (1.0)\n",
      "Current episode: 3690, success: 10 (1.0)\n",
      "Current episode: 3700, success: 10 (1.0)\n",
      "Current episode: 3710, success: 10 (1.0)\n",
      "Current episode: 3720, success: 10 (1.0)\n",
      "Current episode: 3730, success: 10 (1.0)\n",
      "Current episode: 3740, success: 10 (1.0)\n",
      "Current episode: 3750, success: 10 (1.0)\n",
      "Current episode: 3760, success: 10 (1.0)\n",
      "Current episode: 3770, success: 10 (1.0)\n",
      "Current episode: 3780, success: 10 (1.0)\n",
      "Current episode: 3790, success: 10 (1.0)\n",
      "Current episode: 3800, success: 10 (1.0)\n",
      "Current episode: 3810, success: 10 (1.0)\n",
      "Current episode: 3820, success: 10 (1.0)\n",
      "Current episode: 3830, success: 10 (1.0)\n",
      "Current episode: 3840, success: 10 (1.0)\n",
      "Current episode: 3850, success: 10 (1.0)\n",
      "Current episode: 3860, success: 10 (1.0)\n",
      "Current episode: 3870, success: 10 (1.0)\n",
      "Current episode: 3880, success: 10 (1.0)\n",
      "Current episode: 3890, success: 10 (1.0)\n",
      "Current episode: 3900, success: 10 (1.0)\n",
      "Current episode: 3910, success: 10 (1.0)\n",
      "Current episode: 3920, success: 10 (1.0)\n",
      "Current episode: 3930, success: 10 (1.0)\n",
      "Current episode: 3940, success: 10 (1.0)\n",
      "Current episode: 3950, success: 10 (1.0)\n",
      "Current episode: 3960, success: 10 (1.0)\n",
      "Current episode: 3970, success: 10 (1.0)\n",
      "Current episode: 3980, success: 10 (1.0)\n",
      "Current episode: 3990, success: 10 (1.0)\n",
      "Current episode: 4000, success: 10 (1.0)\n",
      "Current episode: 4010, success: 10 (1.0)\n",
      "Current episode: 4020, success: 10 (1.0)\n",
      "Current episode: 4030, success: 10 (1.0)\n",
      "Current episode: 4040, success: 10 (1.0)\n",
      "Current episode: 4050, success: 10 (1.0)\n",
      "Current episode: 4060, success: 10 (1.0)\n",
      "Current episode: 4070, success: 10 (1.0)\n",
      "Current episode: 4080, success: 10 (1.0)\n",
      "Current episode: 4090, success: 10 (1.0)\n",
      "Current episode: 4100, success: 10 (1.0)\n",
      "Current episode: 4110, success: 10 (1.0)\n",
      "Current episode: 4120, success: 10 (1.0)\n",
      "Current episode: 4130, success: 10 (1.0)\n",
      "Current episode: 4140, success: 10 (1.0)\n",
      "Current episode: 4150, success: 10 (1.0)\n",
      "Current episode: 4160, success: 10 (1.0)\n",
      "Current episode: 4170, success: 10 (1.0)\n",
      "Current episode: 4180, success: 10 (1.0)\n",
      "Current episode: 4190, success: 10 (1.0)\n",
      "Current episode: 4200, success: 10 (1.0)\n",
      "Current episode: 4210, success: 10 (1.0)\n",
      "Current episode: 4220, success: 10 (1.0)\n",
      "Current episode: 4230, success: 10 (1.0)\n",
      "Current episode: 4240, success: 10 (1.0)\n",
      "Current episode: 4250, success: 10 (1.0)\n",
      "Current episode: 4260, success: 10 (1.0)\n",
      "Current episode: 4270, success: 10 (1.0)\n",
      "Current episode: 4280, success: 10 (1.0)\n",
      "Current episode: 4290, success: 10 (1.0)\n",
      "Current episode: 4300, success: 10 (1.0)\n",
      "Current episode: 4310, success: 10 (1.0)\n",
      "Current episode: 4320, success: 10 (1.0)\n",
      "Current episode: 4330, success: 10 (1.0)\n",
      "Current episode: 4340, success: 10 (1.0)\n",
      "Current episode: 4350, success: 10 (1.0)\n",
      "Current episode: 4360, success: 10 (1.0)\n",
      "Current episode: 4370, success: 10 (1.0)\n",
      "Current episode: 4380, success: 10 (1.0)\n",
      "Current episode: 4390, success: 10 (1.0)\n",
      "Current episode: 4400, success: 10 (1.0)\n",
      "Current episode: 4410, success: 10 (1.0)\n",
      "Current episode: 4420, success: 10 (1.0)\n",
      "Current episode: 4430, success: 10 (1.0)\n",
      "Current episode: 4440, success: 10 (1.0)\n",
      "Current episode: 4450, success: 10 (1.0)\n",
      "Current episode: 4460, success: 10 (1.0)\n",
      "Current episode: 4470, success: 10 (1.0)\n",
      "Current episode: 4480, success: 10 (1.0)\n",
      "Current episode: 4490, success: 10 (1.0)\n",
      "Current episode: 4500, success: 10 (1.0)\n",
      "Current episode: 4510, success: 10 (1.0)\n",
      "Current episode: 4520, success: 10 (1.0)\n",
      "Current episode: 4530, success: 10 (1.0)\n",
      "Current episode: 4540, success: 10 (1.0)\n",
      "Current episode: 4550, success: 10 (1.0)\n",
      "Current episode: 4560, success: 10 (1.0)\n",
      "Current episode: 4570, success: 10 (1.0)\n",
      "Current episode: 4580, success: 10 (1.0)\n",
      "Current episode: 4590, success: 10 (1.0)\n",
      "Current episode: 4600, success: 10 (1.0)\n",
      "Current episode: 4610, success: 10 (1.0)\n",
      "Current episode: 4620, success: 10 (1.0)\n",
      "Current episode: 4630, success: 10 (1.0)\n",
      "Current episode: 4640, success: 10 (1.0)\n",
      "Current episode: 4650, success: 10 (1.0)\n",
      "Current episode: 4660, success: 10 (1.0)\n",
      "Current episode: 4670, success: 10 (1.0)\n",
      "Current episode: 4680, success: 10 (1.0)\n",
      "Current episode: 4690, success: 10 (1.0)\n",
      "Current episode: 4700, success: 10 (1.0)\n",
      "Current episode: 4710, success: 10 (1.0)\n",
      "Current episode: 4720, success: 10 (1.0)\n",
      "Current episode: 4730, success: 10 (1.0)\n",
      "Current episode: 4740, success: 10 (1.0)\n",
      "Current episode: 4750, success: 10 (1.0)\n",
      "Current episode: 4760, success: 10 (1.0)\n",
      "Current episode: 4770, success: 10 (1.0)\n",
      "Current episode: 4780, success: 10 (1.0)\n",
      "Current episode: 4790, success: 10 (1.0)\n",
      "Current episode: 4800, success: 10 (1.0)\n",
      "Current episode: 4810, success: 10 (1.0)\n",
      "Current episode: 4820, success: 10 (1.0)\n",
      "Current episode: 4830, success: 10 (1.0)\n",
      "Current episode: 4840, success: 10 (1.0)\n",
      "Current episode: 4850, success: 10 (1.0)\n",
      "Current episode: 4860, success: 10 (1.0)\n",
      "Current episode: 4870, success: 10 (1.0)\n",
      "Current episode: 4880, success: 10 (1.0)\n",
      "Current episode: 4890, success: 10 (1.0)\n",
      "Current episode: 4900, success: 10 (1.0)\n",
      "Current episode: 4910, success: 10 (1.0)\n",
      "Current episode: 4920, success: 10 (1.0)\n",
      "Current episode: 4930, success: 10 (1.0)\n",
      "Current episode: 4940, success: 10 (1.0)\n",
      "Current episode: 4950, success: 10 (1.0)\n",
      "Current episode: 4960, success: 10 (1.0)\n",
      "Current episode: 4970, success: 10 (1.0)\n",
      "Current episode: 4980, success: 10 (1.0)\n",
      "Current episode: 4990, success: 10 (1.0)\n",
      "Current episode: 5000, success: 10 (1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "episode = 0\n",
    "success_count = 0\n",
    "\n",
    "# Loop through the required number of episodes\n",
    "while episode<EPISODES:\n",
    "    episode+=1\n",
    "    done = False\n",
    "\n",
    "    # Run the game.  If we are local, display render animation at SHOW_EVERY\n",
    "    # intervals. \n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        print(f\"Current episode: {episode}, success: {success_count} ({float(success_count)/SHOW_EVERY})\")\n",
    "        success = run_game(q_table, True, False)\n",
    "        success_count = 0\n",
    "    else:\n",
    "        success = run_game(q_table, False, True)\n",
    "        \n",
    "    # Count successes\n",
    "    if success:\n",
    "        success_count += 1\n",
    "\n",
    "    # Move epsilon towards its ending value, if it still needs to move\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon = max(0, epsilon - epsilon_change)\n",
    "\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDHTQkREFRSE"
   },
   "source": [
    "As you can see, the number of successful episodes generally increases as training progresses.  It is not advisable to stop the first time that we observe 100% success over 1,000 episodes. There is a randomness to most games, so it is not likely that an agent would retain its 100% success rate with a new run.  Once you observe that the agent has gotten 100% for several update intervals, it might be safe to stop training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMPR70U0sY7o"
   },
   "source": [
    "# Running and Observing the Agent\n",
    "\n",
    "Now that the algorithm has trained the agent, we can observe the agent in action. You can use the following code to see the agent in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "v_mf3A0h-Nox",
    "outputId": "e138d005-a862-441a-feea-e948000486bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_game(q_table, True, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeQzNFSosdys"
   },
   "source": [
    "# Inspecting the Q-Table\n",
    "\n",
    "We can also display the Q-table.  The following code shows the action that the agent would perform for each environment state.  As the weights of a neural network, this table is not straightforward to interpret.  Some patterns do emerge in that directions do arise, as seen by calculating the means of rows and columns. The actions seem consistent at upper and lower halves of both velocity and position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "L62lyCTr-Noz",
    "outputId": "3d69c4b5-48fa-44b0-8168-666d262a7c7a"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(20, 20, 20, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-6739bc5c0eb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34mf'v-{x}'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDISCRETE_GRID_SIZE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\UserFiles\\anaconda\\envs\\ia\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    556\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\UserFiles\\anaconda\\envs\\ia\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[1;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;31m# by definition an array here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\UserFiles\\anaconda\\envs\\ia\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input. shape=(20, 20, 20, 2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(q_table.argmax(axis=2))\n",
    "\n",
    "df.columns = [f'v-{x}' for x in range(DISCRETE_GRID_SIZE[0])]\n",
    "df.index = [f'p-{x}' for x in range(DISCRETE_GRID_SIZE[1])]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z-ZOL-d5Jbon",
    "outputId": "68cd2fb1-3491-4374-d7b3-fa37b1f7d08b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v-0    0.7\n",
       "v-1    0.6\n",
       "v-2    0.7\n",
       "v-3    0.6\n",
       "v-4    0.3\n",
       "v-5    1.1\n",
       "v-6    1.1\n",
       "v-7    1.3\n",
       "v-8    1.3\n",
       "v-9    1.6\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOGA4HP9KHW3",
    "outputId": "84f3f09d-7384-4240-81ac-73af32a0b04c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p-0    1.0\n",
       "p-1    0.9\n",
       "p-2    1.2\n",
       "p-3    0.8\n",
       "p-4    0.9\n",
       "p-5    1.1\n",
       "p-6    0.5\n",
       "p-7    0.7\n",
       "p-8    1.2\n",
       "p-9    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2FmBt7dlQYZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Tarea Qlearningreinforcement.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
