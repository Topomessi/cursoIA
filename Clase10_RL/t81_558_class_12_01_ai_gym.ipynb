{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UImTzmGTIeX9"
   },
   "source": [
    "# Part 12.1: Introduction to the OpenAI Gym\n",
    "\n",
    "[OpenAI Gym](https://gym.openai.com/) aims to provide an easy-to-setup general-intelligence benchmark with a wide variety of different environments. The goal is to standardize how environments are defined in AI research publications so that published research becomes more easily reproducible. The project claims to provide the user with a simple interface. As of June 2017, developers can only use Gym with Python. \n",
    "\n",
    "OpenAI gym is pip-installed onto your local machine.  There are a few significant limitations to be aware of:\n",
    "\n",
    "* OpenAI Gym Atari only **directly** supports Linux and Macintosh\n",
    "* OpenAI Gym Atari can be used with Windows; however, it requires a particular [installation procedure](https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30)\n",
    "* OpenAI Gym can not directly render animated games in Google CoLab.\n",
    "\n",
    "Because OpenAI Gym requires a graphics display, the only way to display Gym in Google CoLab is an embedded video.  The presentation of OpenAI Gym game animations in Google CoLab is discussed later in this module.\n",
    "\n",
    "### OpenAI Gym Leaderboard\n",
    "\n",
    "The OpenAI Gym does have a leaderboard, similar to Kaggle; however, the OpenAI Gym's leaderboard is much more informal compared to Kaggle.  The user's local machine performs all scoring.  As a result, the OpenAI gym's leaderboard is strictly an \"honor's system.\"  The leaderboard is maintained the following GitHub repository:\n",
    "\n",
    "* [OpenAI Gym Leaderboard](https://github.com/openai/gym/wiki/Leaderboard)\n",
    "\n",
    "If you submit a score, you are required to provide a writeup with sufficient instructions to reproduce your result. A video of your results is suggested, but not required.\n",
    "\n",
    "### Looking at Gym Environments\n",
    "\n",
    "The centerpiece of Gym is the environment, which defines the \"game\" in which your reinforcement algorithm will compete.  An environment does not need to be a game; however, it describes the following game-like features:\n",
    "* **action space**: What actions can we take on the environment, at each step/episode, to alter the environment.\n",
    "* **observation space**: What is the current state of the portion of the environment that we can observe. Usually, we can see the entire environment.\n",
    "\n",
    "Before we begin to look at Gym, it is essential to understand some of the terminology used by this library.\n",
    "\n",
    "* **Agent** - The machine learning program or model that controls the actions.\n",
    "Step - One round of issuing actions that affect the observation space.\n",
    "* **Episode** - A collection of steps that terminates when the agent fails to meet the environment's objective, or the episode reaches the maximum number of allowed steps.\n",
    "* **Render** - Gym can render one frame for display after each episode.\n",
    "* **Reward** - A positive reinforcement that can occur at the end of each episode, after the agent acts.\n",
    "* **Nondeterministic** - For some environments, randomness is a factor in deciding what effects actions have on reward and changes to the observation space.\n",
    "\n",
    "It is important to note that many of the gym environments specify that they are not nondeterministic even though they make use of random numbers to process actions. It is generally agreed upon (based on the gym GitHub issue tracker) that nondeterministic property means that a deterministic environment will still behave randomly even when given consistent seed value. The seed method of an environment can be used by the program to seed the random number generator for the environment.\n",
    "\n",
    "The Gym library allows us to query some of these attributes from environments.  I created the following function to query gym environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cciTuR2MIeX-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UserFiles\\anaconda\\envs\\ia\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\UserFiles\\anaconda\\envs\\ia\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "D:\\UserFiles\\anaconda\\envs\\ia\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\\n%s\" %\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "def query_environment(name):\n",
    "  env = gym.make(name)\n",
    "  spec = gym.spec(name)\n",
    "  print(f\"Action Space: {env.action_space}\")\n",
    "  print(f\"Observation Space: {env.observation_space}\")\n",
    "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
    "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
    "  print(f\"Reward Range: {env.reward_range}\")\n",
    "  print(f\"Reward Threshold: {spec.reward_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Kojoley/atari-py.git\n",
      "  Cloning https://github.com/Kojoley/atari-py.git to c:\\users\\yoda\\appdata\\local\\temp\\pip-req-build-87ty7yug\n",
      "Requirement already satisfied: numpy in d:\\userfiles\\anaconda\\envs\\ia\\lib\\site-packages (from atari-py==1.2.2) (1.19.2)\n",
      "Building wheels for collected packages: atari-py\n",
      "  Building wheel for atari-py (setup.py): started\n",
      "  Building wheel for atari-py (setup.py): still running...\n",
      "  Building wheel for atari-py (setup.py): still running...\n",
      "  Building wheel for atari-py (setup.py): finished with status 'done'\n",
      "  Created wheel for atari-py: filename=atari_py-1.2.2-cp38-cp38-win_amd64.whl size=551041 sha256=f5ddc0cf424258b3f0fea3cfff92f735c02ef2ab42f99c3479f82a762bb69411\n",
      "  Stored in directory: C:\\Users\\yoda\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-t9du4tfq\\wheels\\25\\11\\0c\\d37ea19ecec588ab95c4199a485d3a4de5284e9a08b89c8f3f\n",
      "Successfully built atari-py\n",
      "Installing collected packages: atari-py\n",
      "Successfully installed atari-py-1.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/Kojoley/atari-py.git 'C:\\Users\\yoda\\AppData\\Local\\Temp\\pip-req-build-87ty7yug'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Kojoley/atari-py.git\n",
      "  Cloning https://github.com/Kojoley/atari-py.git to c:\\users\\yoda\\appdata\\local\\temp\\pip-req-build-oc5fhkx2\n",
      "Requirement already satisfied: numpy in d:\\userfiles\\anaconda\\envs\\ia\\lib\\site-packages (from atari-py==1.2.2) (1.19.2)\n",
      "Building wheels for collected packages: atari-py\n",
      "  Building wheel for atari-py (setup.py): started\n",
      "  Building wheel for atari-py (setup.py): still running...\n",
      "  Building wheel for atari-py (setup.py): still running...\n",
      "  Building wheel for atari-py (setup.py): finished with status 'done'\n",
      "  Created wheel for atari-py: filename=atari_py-1.2.2-cp38-cp38-win_amd64.whl size=551042 sha256=e5f6914c3402cb6b5ed671160c8c14249089a93a6a6227f339a9d40ac8d32073\n",
      "  Stored in directory: C:\\Users\\yoda\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-i6ddlxi6\\wheels\\25\\11\\0c\\d37ea19ecec588ab95c4199a485d3a4de5284e9a08b89c8f3f\n",
      "Successfully built atari-py\n",
      "Installing collected packages: atari-py\n",
      "Successfully installed atari-py-1.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/Kojoley/atari-py.git 'C:\\Users\\yoda\\AppData\\Local\\Temp\\pip-req-build-oc5fhkx2'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/Kojoley/atari-py.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Box2D\n",
      "  Downloading Box2D-2.3.10-cp38-cp38-win_amd64.whl (1.3 MB)\n",
      "Installing collected packages: Box2D\n",
      "Successfully installed Box2D-2.3.10\n"
     ]
    }
   ],
   "source": [
    "!pip install Box2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRXfAdwFDwUm"
   },
   "source": [
    "We will begin by looking at the MountainCar-v0 environment, which challenges an underpowered car to escape the valley between two mountains.  The following code describes the Mountian Car environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "XYwy9cjlJjEH",
    "outputId": "e2ab1b90-55db-422a-ef77-71227863cb35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(3)\n",
      "Observation Space: Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)\n",
      "Max Episode Steps: 200\n",
      "Nondeterministic: False\n",
      "Reward Range: (-inf, inf)\n",
      "Reward Threshold: -110.0\n"
     ]
    }
   ],
   "source": [
    "query_environment(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TsQiaGJE3UA"
   },
   "source": [
    "There are three distinct actions that can be taken: accelrate forward, decelerate, or accelerate backwards.  The observation space contains two continuous (floating point) values, as evident by the box object. The observation space is simply the position and velocity of the car.  The car has 200 steps to escape for each epasode.  You would have to look at the code to know, but the mountian car recieves no incramental reward.  The only reward for the car is given when it escapes the valley.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "RF4n5cYEMyru",
    "outputId": "7a8b73c7-6c56-4700-f866-1dfeec2a4080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(2)\n",
      "Observation Space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Max Episode Steps: 500\n",
      "Nondeterministic: False\n",
      "Reward Range: (-inf, inf)\n",
      "Reward Threshold: 475.0\n"
     ]
    }
   ],
   "source": [
    "query_environment(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwvVKrNebUHJ"
   },
   "source": [
    "The CartPole-v1 environment challenges the agent to move a cart while keeping a pole balanced. The environment has an observation space of 4 continuous numbers:\n",
    "\n",
    "* Cart Position\n",
    "* Cart Velocity\n",
    "* Pole Angle\n",
    "* Pole Velocity At Tip\n",
    "\n",
    "To achieve this goal, the agent can take the following actions:\n",
    "\n",
    "* Push cart to the left\n",
    "* Push cart to the right\n",
    "\n",
    "There is also a continuous variant of the mountain car.  This version does not simply have the motor on or off.  For the continuous car the action space is a single floating point number that specifies how much forward or backward force is being applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "UAlaMcJmNSY0",
    "outputId": "456d2977-9939-40c1-ad13-4576eeff0b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Box(-1.0, 1.0, (1,), float32)\n",
      "Observation Space: Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)\n",
      "Max Episode Steps: 999\n",
      "Nondeterministic: False\n",
      "Reward Range: (-inf, inf)\n",
      "Reward Threshold: 90.0\n"
     ]
    }
   ],
   "source": [
    "query_environment(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBrlG1t6ceIa"
   },
   "source": [
    "Note: ignore the warning above, it is a relativly inconsequential bug in OpenAI Gym.\n",
    "\n",
    "Atari games, like breakout can use an observation space that is either equal to the size of the Atari screen (210x160) or even use the RAM memory of the Atari (128 bytes) to determine the state of the game.  Yes thats bytes, not kilobytes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "ndTb-9pgJizW",
    "outputId": "8d1bf60a-ace3-4a6c-ac5b-c2f0c70cc319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(4)\n",
      "Observation Space: Box(0, 255, (210, 160, 3), uint8)\n",
      "Max Episode Steps: 10000\n",
      "Nondeterministic: False\n",
      "Reward Range: (-inf, inf)\n",
      "Reward Threshold: None\n"
     ]
    }
   ],
   "source": [
    "query_environment(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Ni1rxzmLKAdH",
    "outputId": "4066d9e0-882b-49b4-d399-e3a82c3a2a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(4)\n",
      "Observation Space: Box(0, 255, (128,), uint8)\n",
      "Max Episode Steps: 10000\n",
      "Nondeterministic: False\n",
      "Reward Range: (-inf, inf)\n",
      "Reward Threshold: None\n"
     ]
    }
   ],
   "source": [
    "query_environment(\"Breakout-ram-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
